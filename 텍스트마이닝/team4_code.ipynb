{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c54aba26",
   "metadata": {},
   "source": [
    "Î™®Îç∏ ÌïôÏäµÏΩîÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f8119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "\n",
    "# ================================\n",
    "# ‚úÖ ÌôòÍ≤Ω ÏÑ§Ï†ï\n",
    "# ================================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"kykim/bert-kor-base\")  # KorSci-BERT\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"kykim/bert-kor-base\",\n",
    "    num_labels=2,\n",
    "    use_safetensors=True  # Î≥¥Ïïà Ïù¥Ïäà Ïö∞Ìöå\n",
    ").to(device)\n",
    "\n",
    "# ================================\n",
    "# ‚úÖ Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
    "# ================================\n",
    "train_dataset = ConcatDataset([\n",
    "    ClauseDataset('T_P', tokenizer, 1),\n",
    "    ClauseDataset('T_N', tokenizer, 0)\n",
    "])\n",
    "val_dataset = ConcatDataset([\n",
    "    ClauseDataset('V_P', tokenizer, 1),\n",
    "    ClauseDataset('V_N', tokenizer, 0)\n",
    "])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# ================================\n",
    "# ‚úÖ ÌÅ¥ÎûòÏä§ Î∂àÍ∑†Ìòï Î≥¥Ï†ï Loss ÏÑ§Ï†ï\n",
    "# ================================\n",
    "pos_count = 5932\n",
    "neg_count = 3172\n",
    "total = pos_count + neg_count\n",
    "\n",
    "# Î∂àÎ¶¨(label=0), Ïú†Î¶¨(label=1)\n",
    "class_weights = torch.tensor([total / neg_count, total / pos_count], dtype=torch.float32).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# ÏòµÌã∞ÎßàÏù¥Ï†Ä ÏÑ§Ï†ï\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# ================================\n",
    "# ‚úÖ ÌïôÏäµ Î£®ÌîÑ Ìï®Ïàò\n",
    "# ================================\n",
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Train\"):\n",
    "        input_ids, attn_mask, labels = [x.to(device) for x in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "# ================================\n",
    "# ‚úÖ Í≤ÄÏ¶ù Î£®ÌîÑ Ìï®Ïàò\n",
    "# ================================\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Val\"):\n",
    "            input_ids, attn_mask, labels = [x.to(device) for x in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"Î∂àÎ¶¨\", \"Ïú†Î¶¨\"], digits=4)\n",
    "    return total_loss / len(dataloader), correct / total, report\n",
    "\n",
    "# ================================\n",
    "# ‚úÖ ÌïôÏäµ Ïã§Ìñâ (ÏµúÍ≥† Î™®Îç∏ Ï†ÄÏû• Ìè¨Ìï®)\n",
    "# ================================\n",
    "best_acc = 0.0\n",
    "early_stop_count = 0\n",
    "EARLY_STOP = 3\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\nüîÅ Epoch {epoch}\")\n",
    "    \n",
    "    # üîπ ÌïôÏäµ\n",
    "    train_loss, train_acc = train(model, train_loader)\n",
    "    \n",
    "    # üîπ Í≤ÄÏ¶ù\n",
    "    val_loss, val_acc, val_report = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"üìò Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n",
    "    print(f\"üìó Val Loss:   {val_loss:.4f} | Acc: {val_acc:.4f}\")\n",
    "    print(f\"\\n{val_report}\")\n",
    "\n",
    "    # ‚úÖ ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏ Ï†ÄÏû•\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_korsci_bert_model.pt\")\n",
    "        print(\"‚úÖ Best model saved.\")\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "        print(f\"‚è∏ No improvement. Early stop count: {early_stop_count}\")\n",
    "\n",
    "    # ‚õîÔ∏è Ï°∞Í∏∞ Ï¢ÖÎ£å Ï°∞Í±¥\n",
    "    if early_stop_count >= EARLY_STOP:\n",
    "        print(\"‚õîÔ∏è Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06168a4",
   "metadata": {},
   "source": [
    "SHAP Î∂ÑÏÑù ÏΩîÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e06802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "\n",
    "# ‚úÖ Î™®Îç∏ ÏÑ§Ï†ï\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"./domain_model\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ‚úÖ stopwords Î∂àÎü¨Ïò§Í∏∞\n",
    "with open(\"stopwords_ko.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    STOPWORDS = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "# ‚úÖ Ïú†Ìã∏ Ìï®Ïàò\n",
    "def is_structural_phrase(word):\n",
    "    return re.match(r'^Ï†ú\\d+Ìï≠$', word) or re.match(r'^Ï†ú\\d+Ï°∞$', word) or re.match(r'^Ï†ú\\d+$', word) or re.match(r'^\\d+$', word) or word in {'‚ë†','‚ë°','‚ë¢','‚ë£','‚ë§','‚ë•','‚ë¶'}\n",
    "\n",
    "def remove_clause_title(text):\n",
    "    lines = text.strip().split(\"\\n\")\n",
    "    return \"\\n\".join(lines[1:]).strip() if re.match(r'^Ï†ú\\d+Ï°∞', lines[0]) else text\n",
    "\n",
    "def load_all_clauses(folders):\n",
    "    clauses = []\n",
    "    for folder in folders:\n",
    "        for fname in os.listdir(folder):\n",
    "            if fname.endswith(\".json\"):\n",
    "                with open(os.path.join(folder, fname), 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    clauses.extend(clause.strip() for clause in data.get(\"clauseArticle\", []) if isinstance(clause, str))\n",
    "    return clauses\n",
    "\n",
    "# ‚úÖ SHAPÏö© ÎûòÌçº\n",
    "class BertWrapper:\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "    def __call__(self, texts):\n",
    "        inputs = self.tokenizer(list(texts), return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        input_ids = inputs['input_ids'].to(self.device)\n",
    "        attention_mask = inputs['attention_mask'].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "# ‚úÖ SHAP Îã®Ïñ¥ Î≥ëÌï©\n",
    "def merge_tokens_by_offset(text, shap_values, tokenizer, label_idx):\n",
    "    encoding = tokenizer(text, return_offsets_mapping=True, return_tensors='pt', truncation=True, max_length=128)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
    "    offsets = encoding['offset_mapping'][0].tolist()\n",
    "    values = shap_values.values[0][:, label_idx]\n",
    "    words, scores, current_word, current_score, prev_end = [], [], '', 0.0, -1\n",
    "\n",
    "    for token, (start, end), score in zip(tokens, offsets, values):\n",
    "        if token in ['[CLS]', '[SEP]'] or start == end:\n",
    "            continue\n",
    "        if start != prev_end and current_word:\n",
    "            words.append(current_word)\n",
    "            scores.append(current_score)\n",
    "            current_word, current_score = '', 0.0\n",
    "        current_word += text[start:end]\n",
    "        current_score += score\n",
    "        prev_end = end\n",
    "\n",
    "    if current_word:\n",
    "        words.append(current_word)\n",
    "        scores.append(current_score)\n",
    "\n",
    "    filtered = [(w.strip(), s) for w, s in zip(words, scores)\n",
    "                if len(w) > 1 and not re.match(r'^[\\W\\d]+$', w) and w not in STOPWORDS and not is_structural_phrase(w)]\n",
    "    return filtered\n",
    "\n",
    "# ‚úÖ ÌïµÏã¨ Íµ¨ Ï∂îÏ∂ú\n",
    "def extract_phrases_with_scores(sentence, top_words, word_score_dict, window=8):\n",
    "    words = re.findall(r'[Í∞Ä-Ìû£a-zA-Z0-9]+', sentence)\n",
    "    seen, phrases = set(), []\n",
    "    for i in range(len(words)):\n",
    "        for j in range(i+1, min(len(words), i+window)+1):\n",
    "            chunk = ' '.join(words[i:j]).strip()\n",
    "            if chunk in seen: continue\n",
    "            seen.add(chunk)\n",
    "            matched = [w for w in top_words if w in chunk]\n",
    "            if len(matched) >= 1:\n",
    "                score_sum = sum(word_score_dict.get(w, 0.0) for w in matched)\n",
    "                phrases.append((chunk, score_sum))\n",
    "    phrases.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    return phrases[:1]\n",
    "\n",
    "# ‚úÖ Î≥µÌï©Î™ÖÏÇ¨ Í∏∞Î∞ò n-gram\n",
    "def compute_ngram_shap_sum(nouns, word_score_dict, label_text, n=3):\n",
    "    def contains_stopword(phrase): return any(sw in phrase for sw in {\"Ï†ú\",\"Ï°∞\",\"Ìï≠\",\"Ìò∏\",\"Î≤àÌò∏\"}) or re.search(r'[^\\wÍ∞Ä-Ìû£ ]', phrase)\n",
    "    filtered = [n for n in nouns if not contains_stopword(n) and not is_structural_phrase(n)]\n",
    "    ngram_score_sum = {}\n",
    "    for i in range(len(filtered) - n + 1):\n",
    "        ngram = ' '.join(filtered[i:i+n])\n",
    "        if ngram in ngram_score_sum: continue\n",
    "        score_sum = sum(word_score_dict.get(w, 0.0) for w in filtered[i:i+n])\n",
    "        if not contains_stopword(ngram) and score_sum > 0:\n",
    "            ngram_score_sum[ngram] = score_sum\n",
    "    results = sorted(ngram_score_sum.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    seen, final = set(), []\n",
    "    for phrase, score in results:\n",
    "        if any(p in seen for p in phrase.split()): continue\n",
    "        final.append((phrase, score))\n",
    "        seen.update(phrase.split())\n",
    "        if len(final) >= 3: break\n",
    "    print(f\"\\nüìå SHAP Í∏∞Ï§Ä, '{label_text}' ÌåêÎã®Ïóê Í∞ÄÏû• ÌÅ∞ ÏòÅÌñ•ÏùÑ Ï§Ä {n}-gram ÌëúÌòÑ (Î≥µÌï©Î™ÖÏÇ¨ Í∏∞Î∞ò):\")\n",
    "    for phrase, score in final:\n",
    "        print(f\"  - '{phrase}': SUM = {score:.4f}\")\n",
    "\n",
    "# ‚úÖ Î≥µÌï©Î™ÖÏÇ¨ Ï∂îÏ∂úÍ∏∞ ÌïôÏäµ\n",
    "all_folders = [\"C:/data/TL_Ïú†Î¶¨\", \"C:/data/TL_Î∂àÎ¶¨\", \"C:/data/VL_Ïú†Î¶¨\", \"C:/data/VL_Î∂àÎ¶¨\"]\n",
    "corpus = load_all_clauses(all_folders)\n",
    "with open(\"processed_clauses.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus += [line.strip() for line in f if line.strip()]\n",
    "print(f\"‚úÖ Ï†ÑÏ≤¥ ÌïôÏäµ Î¨∏Ïû• Ïàò: {len(corpus)}\")\n",
    "noun_extractor = LRNounExtractor_v2(verbose=False)\n",
    "noun_extractor.train(corpus)\n",
    "nouns_score_dict = noun_extractor.extract()\n",
    "\n",
    "# ‚úÖ ÏûÖÎ†• Î¨∏Ïû• ÏßÅÏ†ë ÏßÄÏ†ï\n",
    "input_text = \"‚ë° Ïù¥ Í≤ΩÏö∞ ÌöåÏÇ¨Í∞Ä ÌöåÏõêÏóêÍ≤å ÌôòÍ∏âÏùÑ ÏßÄÏó∞Ìïú ÎïåÏóêÎäî Í∑∏ ÏßÄÏó∞Í∏∞Í∞ÑÏóê ÎåÄÌïòÏó¨ Ï†ÑÏûêÏÉÅÍ±∞Îûò Îì±ÏóêÏÑúÏùò ÏÜåÎπÑÏûêÎ≥¥Ìò∏Ïóê Í¥ÄÌïú Î≤ïÎ•† Î∞è ÏãúÌñâÎ†πÏóêÏÑú Ï†ïÌïòÎäî Ïù¥Ïú®ÏùÑ Í≥±ÌïòÏó¨ ÏÇ∞Ï†ïÌïú ÏßÄÏó∞Ïù¥ÏûêÎ•º ÏßÄÍ∏âÌï¥Ïïº Ìï©ÎãàÎã§.\"\n",
    "sentence = remove_clause_title(input_text)\n",
    "\n",
    "# ‚úÖ ÏòàÏ∏° + Ìï¥ÏÑù\n",
    "wrapper = BertWrapper(model, tokenizer, device)\n",
    "probs = wrapper([sentence])[0]\n",
    "label = int(probs.argmax())\n",
    "label_text = 'Ïú†Î¶¨' if label == 1 else 'Î∂àÎ¶¨'\n",
    "print(f\"\\n‚úÖ ÏòàÏ∏° Í≤∞Í≥º: {label_text} ({probs[label]:.4f})\")\n",
    "\n",
    "explainer = shap.Explainer(wrapper, shap.maskers.Text(tokenizer))\n",
    "shap_values = explainer([sentence])\n",
    "shap.plots.text(shap_values[0])\n",
    "\n",
    "merged = merge_tokens_by_offset(sentence, shap_values, tokenizer, label)\n",
    "word_score_dict = defaultdict(float)\n",
    "for w, s in merged:\n",
    "    word_score_dict[w] += s\n",
    "\n",
    "top_words = [w for w, _ in sorted(word_score_dict.items(), key=lambda x: abs(x[1]), reverse=True)[:10]]\n",
    "print(f\"\\nüîç SHAP Í∏∞Ï§Ä ÏÉÅÏúÑ Îã®Ïñ¥: {top_words}\")\n",
    "\n",
    "phrase = extract_phrases_with_scores(sentence, top_words, word_score_dict)\n",
    "print(f\"\\nüìå {label_text} ÌåêÎã® ÌïµÏã¨ Íµ¨(Phrase):\")\n",
    "for p, s in phrase:\n",
    "    print(f\"  - {p}: SUM = {s:.4f}\")\n",
    "\n",
    "nouns = [n for n in nouns_score_dict if n in sentence and not is_structural_phrase(n)]\n",
    "compute_ngram_shap_sum(nouns, word_score_dict, label_text=label_text, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5893d01-764c-422a-bb83-6c676b5e0799",
   "metadata": {},
   "source": [
    "ÏïΩÍ¥Ä Ï°∞Ìï≠ Ï†ÑÏ≤òÎ¶¨ ÏΩîÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f06044-cb51-4ebd-9d13-b81070749614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Íµ¨Í∏Ä ÎìúÎùºÏù¥Î∏å ÎßàÏö¥Ìä∏\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. ÌïÑÏöî Î™®Îìà ÏûÑÌè¨Ìä∏\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# 3. Ï°∞Ìï≠ Î∂ÑÎ¶¨ Ìï®Ïàò (Í∞ÑÎã®Ìûà 'Ï†ú n Ï°∞' Í∏∞Ï§Ä Î∂ÑÎ¶¨)\n",
    "def split_clauses(text):\n",
    "    pattern = re.compile(r'(Ï†ú\\s*\\d+\\s*Ï°∞[^\\n]*)')\n",
    "    parts = pattern.split(text)\n",
    "    clauses = []\n",
    "    for i in range(1, len(parts), 2):\n",
    "        title = parts[i].strip()\n",
    "        content = parts[i+1].strip() if i+1 < len(parts) else ''\n",
    "        full_text = title + \" \" + content\n",
    "        clauses.append(full_text)\n",
    "    return clauses\n",
    "\n",
    "# 4. Î™®Îç∏ Í≤ΩÎ°ú ÏßÄÏ†ï (ÏûêÏã†Ïùò Í≤ΩÎ°úÎ°ú ÏàòÏ†ï)\n",
    "MODEL_PATH = \"/content/drive/MyDrive/domain_model\"\n",
    "\n",
    "# 5. Î™®Îç∏ Î∞è ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 6. Ï°∞Ìï≠Î≥Ñ ÏòàÏ∏° Ìï®Ïàò (softmax ÌôïÎ•† + ÏòàÏ∏° ÎùºÎ≤® Î∞òÌôò)\n",
    "def predict_with_probs(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred_label = torch.argmax(probs, dim=1).item()\n",
    "        prob_vals = probs[0].cpu().tolist()\n",
    "    return pred_label, prob_vals\n",
    "\n",
    "# 7. Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò (Ïó¨Îü¨ ÌååÏùº Ï≤òÎ¶¨)\n",
    "def main():\n",
    "    # ‚úÖ Î∂ÑÏÑùÌï† ÏïΩÍ¥Ä ÌååÏùº Í≤ΩÎ°úÎì§ (ÏûêÏã†Ïùò ÌååÏùº Í≤ΩÎ°ú Î¶¨Ïä§Ìä∏Î°ú ÏàòÏ†ï)\n",
    "    terms_paths = [\n",
    "        '/content/drive/MyDrive/001_Í∞úÏù∏Ï†ïÎ≥¥Ï∑®Í∏âÎ∞©Ïπ®_Í∞ÄÍ≥µ.xml',\n",
    "        '/content/drive/MyDrive/001_Í∞ÄÎßπÍ≥ÑÏïΩ_Í∞ÄÍ≥µ.xml',\n",
    "        '/content/drive/MyDrive/001_Í≤∞ÌòºÏ†ïÎ≥¥ÏÑúÎπÑÏä§_Í∞ÄÍ≥µ.xml',\n",
    "        '/content/drive/MyDrive/001_Í≥µÍ∏âÍ≥ÑÏïΩ_Í∞ÄÍ≥µ.xml'\n",
    "    ]\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    for path in terms_paths:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            full_text = f.read()\n",
    "\n",
    "        clauses = split_clauses(full_text)\n",
    "        results = []\n",
    "\n",
    "        for clause in clauses:\n",
    "            label, probs = predict_with_probs(clause)\n",
    "            results.append({\n",
    "                \"text\": clause,\n",
    "                \"label\": label,\n",
    "                \"prob_unfavorable\": probs[0],  # Î∂àÎ¶¨ ÌôïÎ•†\n",
    "                \"prob_favorable\": probs[1]    # Ïú†Î¶¨ ÌôïÎ•†\n",
    "            })\n",
    "\n",
    "        filename = os.path.basename(path)\n",
    "        all_results[filename] = results\n",
    "\n",
    "        # üîç Top Ï°∞Ìï≠ Ï∂úÎ†•\n",
    "        top_favorable = sorted(results, key=lambda x: x['prob_favorable'], reverse=True)[:3]\n",
    "        top_unfavorable = sorted(results, key=lambda x: x['prob_unfavorable'], reverse=True)[:3]\n",
    "\n",
    "        print(f\"\\nüìÑ {filename} Í≤∞Í≥º:\")\n",
    "        print(\"=== ‚úÖ Ïú†Î¶¨ ÌôïÎ•† Top 3 ===\")\n",
    "        for i, r in enumerate(top_favorable, 1):\n",
    "            print(f\"{i}. ÌôïÎ•†: {r['prob_favorable']:.4f} | ÌÖçÏä§Ìä∏: {r['text'][:100]}...\")\n",
    "\n",
    "        print(\"=== ‚ö†Ô∏è Î∂àÎ¶¨ ÌôïÎ•† Top 3 ===\")\n",
    "        for i, r in enumerate(top_unfavorable, 1):\n",
    "            print(f\"{i}. ÌôïÎ•†: {r['prob_unfavorable']:.4f} | ÌÖçÏä§Ìä∏: {r['text'][:100]}...\")\n",
    "\n",
    "    # üìù Í≤∞Í≥º JSON Ï†ÄÏû•\n",
    "    output_path = '/content/drive/MyDrive/clause_predictions_all.json'\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\n‚úÖ Ï†ÑÏ≤¥ Í≤∞Í≥º Ï†ÄÏû• ÏôÑÎ£å: {output_path}\")\n",
    "\n",
    "# 8. Ïã§Ìñâ\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e31458",
   "metadata": {},
   "source": [
    "TF-IDF Îã®Ïñ¥ Ï∂îÏ∂ú ÏΩîÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b2116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "from collections import Counter\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ LOGGING ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "logging.basicConfig(\n",
    "    filename=\"terms_extractor_debug.log\",\n",
    "    filemode=\"w\",\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TermsDifficultWordsExtractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        min_word_length: int = 2,\n",
    "        allow_single_char_noun: bool = True,\n",
    "        debug: bool = True,\n",
    "    ):\n",
    "        self.okt = Okt()\n",
    "        self.min_word_length = min_word_length\n",
    "        self.allow_single_char_noun = allow_single_char_noun\n",
    "        self.debug = debug\n",
    "\n",
    "        # ‚îÄ‚îÄ Í∏∞Î≥∏+ÏïΩÏãù TOS Î∂àÏö©Ïñ¥ ‚îÄ‚îÄ\n",
    "        self.stopwords = {\n",
    "            \"ÏùÄ\", \"Îäî\", \"Ïù¥\", \"Í∞Ä\", \"ÏùÑ\", \"Î•º\", \"Ïóê\", \"Ïùò\", \"ÏôÄ\", \"Í≥º\",\n",
    "            \"ÎèÑ\", \"Î°ú\", \"ÏúºÎ°ú\", \"ÏóêÏÑú\", \"ÍπåÏßÄ\", \"Î∂ÄÌÑ∞\", \"Îßå\", \"ÎùºÎèÑ\",\n",
    "            \"Ï°∞Ï∞®\", \"ÎßàÏ†Ä\", \"ÏóêÍ≤å\", \"ÌïúÌÖå\", \"Î∞è\", \"ÎòêÎäî\", \"Í∑∏Î¶¨Í≥†\",\n",
    "            \"ÌïòÏßÄÎßå\", \"Í∑∏Îü¨ÎÇò\", \"Îî∞ÎùºÏÑú\",\n",
    "            \"ÌöåÏÇ¨\", \"ÏÑúÎπÑÏä§\", \"ÌöåÏõê\", \"Ïù¥Ïö©\", \"ÏïΩÍ¥Ä\", \"Í≥†Í∞ù\", \"ÏÇ¨Ïù¥Ìä∏\",\n",
    "            \"ÏõπÏÇ¨Ïù¥Ìä∏\", \"Î≥∏\", \"ÎãπÏÇ¨\", \"ÏÇ¨Ïö©\", \"Ï†ïÎ≥¥\", \"Í¥ÄÎ†®\", \"Ï†úÍ≥µ\",\n",
    "            \"ÏúÑ\", \"Ïù¥Ìïò\", \"Í≤ΩÏö∞\", \"Îïå\", \"ÎÇ¥Ïö©\", \"Î™©Ï†Å\", \"Ï°∞Ìï≠\", \"Ï†úÍ≥µÏûê\",\n",
    "            \"ÌöåÏõêÏõê\",\n",
    "            \"Ìï©ÎãàÎã§\", \"Ìï©ÎãàÎã§.\",\n",
    "        }\n",
    "\n",
    "    # ‚îÄ‚îÄ ÎÇ¥Î∂Ä ÎîîÎ≤ÑÍ∑∏ ‚îÄ‚îÄ\n",
    "    def _dbg(self, msg: str) -> None:\n",
    "        if self.debug:\n",
    "            log.debug(msg)\n",
    "\n",
    "    # ‚îÄ‚îÄ Ï†ÑÏ≤òÎ¶¨: Î™ÖÏÇ¨Îßå ÎÇ®Í∏∞Í∏∞ ‚îÄ‚îÄ\n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        text = re.sub(r\"[^\\w\\sÍ∞Ä-Ìû£]\", \" \", text)\n",
    "        morphs = self.okt.pos(text, stem=True)\n",
    "\n",
    "        tokens: List[str] = []\n",
    "        for word, pos in morphs:\n",
    "            if pos != \"Noun\":\n",
    "                continue\n",
    "            length_ok = len(word) >= self.min_word_length\n",
    "            if self.allow_single_char_noun and len(word) == 1:\n",
    "                length_ok = True\n",
    "            if not length_ok or word in self.stopwords or word.isdigit():\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "\n",
    "        self._dbg(\n",
    "            f\"Preprocessed {len(text)} chars ‚Üí {len(tokens)} tokens \"\n",
    "            f\"(sample: {tokens[:15]})\"\n",
    "        )\n",
    "        return tokens\n",
    "\n",
    "    # ‚îÄ‚îÄ ÌååÏùº ‚Üí Î¨∏ÏÑú Î¶¨Ïä§Ìä∏ ‚îÄ‚îÄ\n",
    "    def load_corpus_from_file(self, path: str, lines_per_doc: int = 5) -> List[str]:\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            lines = [ln.strip() for ln in f if ln.strip()]\n",
    "        docs = [\n",
    "            \" \".join(lines[i : i + lines_per_doc])\n",
    "            for i in range(0, len(lines), lines_per_doc)\n",
    "        ]\n",
    "        self._dbg(f\"Loaded {len(lines)} lines ‚Üí {len(docs)} docs\")\n",
    "        return docs\n",
    "\n",
    "    # ‚îÄ‚îÄ ÌïµÏã¨: TF-IDF Ï∂îÏ∂ú ‚îÄ‚îÄ\n",
    "    def extract_difficult_words(\n",
    "        self,\n",
    "        terms_text: str,\n",
    "        daily_corpus_path: str,\n",
    "        *,\n",
    "        top_n: int = 20,\n",
    "        min_freq: int = 2,\n",
    "        lines_per_doc: int = 5,\n",
    "    ):\n",
    "        daily_raw = self.load_corpus_from_file(daily_corpus_path, lines_per_doc)\n",
    "        terms_tok = self.preprocess_text(terms_text)\n",
    "        daily_tok_list = [self.preprocess_text(doc) for doc in daily_raw]\n",
    "\n",
    "        docs = [\" \".join(terms_tok)] + [\" \".join(t) for t in daily_tok_list]\n",
    "\n",
    "        vec = TfidfVectorizer(\n",
    "            token_pattern=r\"(?u)\\b[\\wÍ∞Ä-Ìû£]+\\b\", max_features=5000, min_df=1\n",
    "        )\n",
    "        tfidf_mat = vec.fit_transform(docs)\n",
    "        feats = vec.get_feature_names_out()\n",
    "        terms_tfidf = tfidf_mat[0].toarray().flatten()\n",
    "        idf = vec.idf_\n",
    "\n",
    "        terms_cnt = Counter(terms_tok)\n",
    "        daily_cnt = Counter(tok for lst in daily_tok_list for tok in lst)\n",
    "\n",
    "        words = []\n",
    "        for i, w in enumerate(feats):\n",
    "            tfidf_val = terms_tfidf[i]\n",
    "            t_freq = terms_cnt.get(w, 0)\n",
    "            d_freq = daily_cnt.get(w, 0)\n",
    "\n",
    "            # ‚òÖ ÌïÑÌÑ∞: ÏùºÏÉÅ ÎπàÎèÑ 10 Ïù¥ÏÉÅÏù¥Î©¥ Ï†úÏô∏ ‚òÖ\n",
    "            if (\n",
    "                t_freq >= min_freq\n",
    "                and tfidf_val > 0\n",
    "                and d_freq < 10            # ‚Üê ÌïµÏã¨ Ï°∞Í±¥\n",
    "            ):\n",
    "                words.append((w, tfidf_val, t_freq, idf[i], t_freq, d_freq))\n",
    "\n",
    "        self._dbg(f\"Candidates kept after daily_freq<10: {len(words)}\")\n",
    "        words.sort(key=lambda x: x[1], reverse=True)\n",
    "        return words[:top_n]\n",
    "\n",
    "    # ‚îÄ‚îÄ Ïã§Ìñâ & Ï∂úÎ†• ‚îÄ‚îÄ\n",
    "    def analyze_and_display(\n",
    "        self,\n",
    "        terms_file: str,\n",
    "        corpus_file: str,\n",
    "        *,\n",
    "        top_n: int = 20,\n",
    "        min_freq: int = 2,\n",
    "        lines_per_doc: int = 5,\n",
    "    ) -> Optional[pd.DataFrame]:\n",
    "        terms_text = open(terms_file, encoding=\"utf-8\").read()\n",
    "\n",
    "        res = self.extract_difficult_words(\n",
    "            terms_text,\n",
    "            corpus_file,\n",
    "            top_n=top_n,\n",
    "            min_freq=min_freq,\n",
    "            lines_per_doc=lines_per_doc,\n",
    "        )\n",
    "        if not res:\n",
    "            print(\"No difficult words found (after daily_freq < 10 filter).\")\n",
    "            return None\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            res, columns=[\"word\", \"tfidf\", \"tf\", \"idf\", \"terms_freq\", \"daily_freq\"]\n",
    "        )\n",
    "        df[\"difficulty\"] = df[\"terms_freq\"] / (df[\"daily_freq\"] + 1)\n",
    "\n",
    "        # Î©îÏù∏ ÌÖåÏù¥Î∏î\n",
    "        print(\"=\" * 90)\n",
    "        print(f\"Top {len(df)} Difficult Words  (daily_freq < 10)\")\n",
    "        print(\"=\" * 90)\n",
    "        print(\n",
    "            f\"{'Rank':<4} {'Word':<20} {'TF-IDF':<9} {'TF':<5} \"\n",
    "            f\"{'IDF':<7} {'TermsFreq':<10} {'DailyFreq':<9} {'Difficulty':<10}\"\n",
    "        )\n",
    "        print(\"-\" * 90)\n",
    "        for i, r in df.iterrows():\n",
    "            print(\n",
    "                f\"{i+1:<4} {r.word:<20} {r.tfidf:<9.4f} {r.tf:<5} \"\n",
    "                f\"{r.idf:<7.4f} {r.terms_freq:<10} {r.daily_freq:<9} \"\n",
    "                f\"{r.difficulty:<10.2f}\"\n",
    "            )\n",
    "\n",
    "        # sparse spotlight(daily_freq == 0)\n",
    "        sparse = df[df[\"daily_freq\"] == 0]\n",
    "        if not sparse.empty:\n",
    "            print(\"\\n‚ñ∫ Sparse words (absent from daily corpus):\")\n",
    "            for w in sparse[\"word\"]:\n",
    "                print(\"  ‚Ä¢\", w)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Driver ‚îÄ‚îÄ\n",
    "def main() -> None:\n",
    "    ext = TermsDifficultWordsExtractor(\n",
    "        min_word_length=4, allow_single_char_noun=False, debug=True\n",
    "    )\n",
    "    ext.analyze_and_display(\n",
    "        terms_file=\"term2.txt\",\n",
    "        corpus_file=\"corpus.txt\",\n",
    "        top_n=15,\n",
    "        min_freq=1,\n",
    "        lines_per_doc=10,\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"Analysis completed.  See 'terms_extractor_debug.log' for details.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245ea90d-6eb5-4f7c-acb5-dc9c4641a2c1",
   "metadata": {},
   "source": [
    "ÎèÑÎ©îÏù∏ Î∂ÑÎ•ò ÏΩîÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4169b7c6-8932-4d08-8501-9c13609db3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "# ============ 1. JSON -> CSV Î≥ÄÌôò Ìï®Ïàò ============\n",
    "def json_dir_to_csv(dir_path, output_csv_path, exclude_fields=None, label=None):\n",
    "    exclude_fields = set(exclude_fields) if exclude_fields else set()\n",
    "    rows = []\n",
    "\n",
    "    for fname in os.listdir(dir_path):\n",
    "        if fname.endswith('.json'):\n",
    "            with open(os.path.join(dir_path, fname), encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            clause_field = data.get('clauseField')\n",
    "            if clause_field and clause_field.isdigit():\n",
    "                if int(clause_field) in exclude_fields:\n",
    "                    continue\n",
    "                text = data.get('clauseArticle', [''])[0]\n",
    "                rows.append({'clauseField': int(clause_field), 'clauseArticle': text, 'label': label})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"CSV saved to {output_csv_path} (samples: {len(df)})\")\n",
    "\n",
    "# ============ 2. ClauseDataset (CSV Í∏∞Î∞ò) ============\n",
    "class ClauseDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.encodings = tokenizer(\n",
    "            self.df['clauseArticle'].tolist(),\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        self.labels = torch.tensor(self.df['label'].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# ============ 3. BertClassifier Ï†ïÏùò ============\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='kykim/bert-kor-base'):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        return self.classifier(cls_output)\n",
    "\n",
    "# ============ 4. ÌïôÏäµ / ÌèâÍ∞Ä Ìï®Ïàò ============\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device, return_all=False):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, mask)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    accuracy = sum([p == l for p, l in zip(all_preds, all_labels)]) / len(all_labels)\n",
    "\n",
    "    if return_all:\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        return accuracy, f1, cm\n",
    "    else:\n",
    "        return accuracy\n",
    "\n",
    "# ============ 5. Î©îÏù∏ Ïã§ÌñâÎ∂Ä ============\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ clauseField (ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Ï†úÏô∏Ìï† ÌïÑÎìú)\n",
    "    val_clause_fields = [1, 2, 3, 5, 6, 7, 8, 11, 25, 27, 28, 29, 30, 31, 32, 33, 39, 43]\n",
    "\n",
    "    # Í≤ΩÎ°ú\n",
    "    base_train_dir  = '/content/ÏïΩÍ¥ÄÎç∞Ïù¥ÌÑ∞/TL_2.ÏïΩÍ¥Ä/TL_2.ÏïΩÍ¥Ä/1.Training/ÎùºÎ≤®ÎßÅÎç∞Ïù¥ÌÑ∞/TL_2.ÏïΩÍ¥Ä'\n",
    "    base_val_dir = '/content/ÏïΩÍ¥ÄÎç∞Ïù¥ÌÑ∞/VL_2.ÏïΩÍ¥Ä/2.Validation/ÎùºÎ≤®ÎßÅÎç∞Ïù¥ÌÑ∞/VL_2.ÏïΩÍ¥Ä'\n",
    "\n",
    "    # JSON -> CSV (ÌïÑÏöîÏãú 1Ìöå Ïã§Ìñâ)\n",
    "    json_dir_to_csv(os.path.join(base_train_dir, '01.Ïú†Î¶¨'), 'train_good.csv', exclude_fields=val_clause_fields, label=1)\n",
    "    json_dir_to_csv(os.path.join(base_train_dir, '02.Î∂àÎ¶¨'), 'train_bad.csv', exclude_fields=val_clause_fields, label=0)\n",
    "    json_dir_to_csv(os.path.join(base_val_dir, '01.Ïú†Î¶¨'), 'val_good.csv', label=1)\n",
    "    json_dir_to_csv(os.path.join(base_val_dir, '02.Î∂àÎ¶¨'), 'val_bad.csv', label=0)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('kykim/bert-kor-base')\n",
    "\n",
    "    train_dataset = ConcatDataset([\n",
    "        ClauseDataset('train_good.csv', tokenizer),\n",
    "        ClauseDataset('train_bad.csv', tokenizer)\n",
    "    ])\n",
    "    val_dataset = ConcatDataset([\n",
    "        ClauseDataset('val_good.csv', tokenizer),\n",
    "        ClauseDataset('val_bad.csv', tokenizer)\n",
    "    ])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, pin_memory=True, num_workers=4)\n",
    "\n",
    "    model = BertClassifier().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # ÌïôÏäµ Î£®ÌîÑ\n",
    "    epochs = 3\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nüåü Epoch {epoch+1}\")\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        val_acc = evaluate(model, val_loader, device)\n",
    "        print(f\"‚úÖ Train Loss: {train_loss:.4f}, üîç Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Ï†ÑÏ≤¥ ÌèâÍ∞Ä ÏßÄÌëú Ï∂úÎ†•\n",
    "    final_acc, final_f1, final_cm = evaluate(model, val_loader, device, return_all=True)\n",
    "    print(\"\\nüìä Ï†ÑÏ≤¥ ÌèâÍ∞Ä ÏßÄÌëú:\")\n",
    "    print(f\"‚úÖ Accuracy: {final_acc:.4f}\")\n",
    "    print(f\"üéØ F1 Score: {final_f1:.4f}\")\n",
    "    print(\"üßÆ Confusion Matrix:\")\n",
    "    print(final_cm)\n",
    "\n",
    "    # Î™®Îç∏ Ï†ÄÏû•\n",
    "    torch.save(model.state_dict(), 'bert_clause_model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f056843",
   "metadata": {},
   "source": [
    "LLM ÌîÑÎ°¨ÌîÑÌä∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf40f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1. ÌôòÍ≤Ω Î≥ÄÏàò & LLM\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise EnvironmentError(\"‚ùó OPENAI_API_KEY ÌôòÍ≤Ω Î≥ÄÏàòÍ∞Ä ÎπÑÏñ¥ ÏûàÏäµÎãàÎã§.\")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\",\n",
    "                 temperature=0.2,\n",
    "                 openai_api_key=api_key)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 2. ÎèÑÎ©îÏù∏ Ï†ïÏùò\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "DOMAIN_CATS = [\n",
    "    \"A. Í∏àÏúµÍ∏∞Í¥Ä\",\"B. Ï†ÑÏûêÏßÄÍ∏â¬∑ÌïÄÌÖåÌÅ¨\",\"C. Î≥¥Ìóò\",\"D. Ï¶ùÍ∂å¬∑Ìà¨Ïûê\",\n",
    "    \"E. Ïú†ÌÜµ¬∑ÏÇ¨Ïù¥Î≤ÑÎ™∞\",\"F. ÌîÑÎûúÏ∞®Ïù¥Ï¶à¬∑Í≥µÍ∏â¬∑Î∂ÑÏñë¬∑Ïã†ÌÉÅ\",\"G. Î∂ÄÎèôÏÇ∞¬∑ÏûÑÎåÄÏ∞®¬∑Î¶¨Ïä§\",\n",
    "    \"H. Ïö¥ÏÜ°¬∑Î¨ºÎ•ò\",\"I. Ïó¨Ìñâ¬∑Î†àÏ†Ä¬∑Í≤åÏûÑ\",\"J. ÏÉùÌôúÏÑúÎπÑÏä§\",\"K. Í∏∞ÌÉÄ Í≥ÑÏïΩ¬∑Î≥¥Ï¶ù\",\n",
    "]\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 3. ÏïΩÍ¥Ä ÏöîÏïΩ + ÎèÑÎ©îÏù∏ Î∂ÑÎ•ò\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "summary_schema = ResponseSchema(\n",
    "    name=\"terms_summary\",\n",
    "    description=\"ÏïΩÍ¥Ä Ï†ÑÎ¨∏ Ïù¥Ìï¥ÌïòÍ∏∞ ÏâΩÎèÑÎ°ù ÏöîÏïΩ\"\n",
    ")\n",
    "domain_schema = ResponseSchema(\n",
    "    name=\"domains\",\n",
    "    description=\"Ï°∞Ìï≠ Î™©Î°ùÍ≥º ÎèôÏùºÌïú ÏàúÏÑúÎ°ú ÎèÑÎ©îÏù∏ Î¨∏ÏûêÏó¥ Î∞∞Ïó¥\"\n",
    ")\n",
    "parser = StructuredOutputParser.from_response_schemas([summary_schema, domain_schema])\n",
    "format_instr = parser.get_format_instructions()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"{format_instr}\n",
    "\n",
    "ÏïΩÍ¥Ä Ï†ÑÎ¨∏:\n",
    "{terms_text}\n",
    "\n",
    "Ï°∞Ìï≠ Î™©Î°ù:\n",
    "{clauses}\n",
    "\n",
    "ÏûëÏóÖ:\n",
    "1) ÏïΩÍ¥Ä Ï†ÑÎ¨∏ÏùÑ ÏÜåÎπÑÏûê Í¥ÄÏ†êÏóêÏÑú Ïù¥Ìï¥ÌïòÍ∏∞ ÏâΩÎèÑÎ°ù ÏöîÏïΩ\n",
    "2) Í∞Å Ï°∞Ìï≠Ïùò ÏÇ∞ÏóÖ¬∑ÏÑúÎπÑÏä§ Î∂ÑÏïº(ÏïÑÎûò 11Í∞ú Ï§ë ÌïòÎÇò)Î•º ÏÑ†ÌÉùÌïòÏó¨ ÏàúÏÑúÎåÄÎ°ú ÎÇòÏó¥\n",
    "\n",
    "Ïπ¥ÌÖåÍ≥†Î¶¨:\n",
    "{domain_list}\n",
    "\"\"\"\n",
    ")\n",
    "summary_chain = LLMChain(llm=llm, prompt=prompt, output_parser=parser)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 4. Ï°∞Ìï≠ Ï≤òÎ¶¨ Ìï®Ïàò (ÏûÑÎ≤†Îî© Ï†úÍ±∞)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def process_clause(clause, label, domain):\n",
    "    prompt = f\"\"\"\n",
    "[Ï°∞Ìï≠ ÏõêÎ¨∏]\n",
    "{clause}\n",
    "\n",
    "ÏûëÏóÖ:\n",
    "1) ÏúÑ Ï°∞Ìï≠Ïù¥ ÏÜåÎπÑÏûêÏóêÍ≤å Ïôú {label}ÌïúÏßÄ 2~3Î¨∏Ïû•ÏúºÎ°ú ÏÑ§Î™ÖÌïòÏÑ∏Ïöî.\n",
    "2) {\"Î∂àÎ¶¨ Ï°∞Ìï≠ÏùÑ ÏÜåÎπÑÏûêÏóêÍ≤å Ïú†Î¶¨ÌïòÍ≤å Í∞úÏ†ïÌïòÏÑ∏Ïöî. ÌòïÏãù: Í∞úÏ†ï Ï†Ñ: / Í∞úÏ†ï ÌõÑ:\" \n",
    "     if label == \"Î∂àÎ¶¨\" \n",
    "     else \"Ïú†Î¶¨ Ï°∞Ìï≠ÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥†, Í∞úÏÑ†Ìï† Î∂ÄÎ∂ÑÏù¥ ÏûàÏúºÎ©¥ Í∞ÑÎã®Ìûà Ï†úÏïàÌïòÏÑ∏Ïöî.\"}\n",
    "3) Í∞ÄÎä•ÌïòÎ©¥ ÏùºÎ∞òÏ†ÅÏù∏ Í¥ÄÎ†® Î≤ïÎ†π ÏòàÏãúÎèÑ Ìï®Íªò Ï†úÍ≥µÌïòÏÑ∏Ïöî.\n",
    "\"\"\"\n",
    "    llm_result = llm.predict(prompt).strip()\n",
    "\n",
    "\n",
    "    # SHAP ÏÑ§Î™Ö ÎåÄÏ≤¥ ÎòêÎäî Ï†úÍ±∞\n",
    "    shap_info = {\"note\": \"SHAP ÏÑ§Î™Ö ÏÉùÎûµÎê® (ÏûÑÎ≤†Îî© Ï†úÍ±∞Îê®)\"}\n",
    "\n",
    "    return {\n",
    "        \"domain\": domain,\n",
    "        \"llm_result\": llm_result,\n",
    "        \"law_refs\": [],\n",
    "        \"shap_explanation\": shap_info\n",
    "    }\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 5. Î©îÏù∏ ÌååÏù¥ÌîÑÎùºÏù∏\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def run_terms_analysis(terms_text: str, clauses: List[str], labels: List[str]):\n",
    "    if len(clauses) != len(labels):\n",
    "        raise ValueError(\"clausesÏôÄ labels Í∏∏Ïù¥Í∞Ä Îã§Î¶ÖÎãàÎã§.\")\n",
    "\n",
    "    # ÏöîÏïΩ + ÎèÑÎ©îÏù∏ Î∂ÑÎ•ò\n",
    "    parsed = summary_chain.predict_and_parse(\n",
    "        terms_text=terms_text,\n",
    "        clauses=\"\\n\".join(f\"{i+1}) {c}\" for i, c in enumerate(clauses)),\n",
    "        format_instr=format_instr,\n",
    "        domain_list=\"\\n\".join(DOMAIN_CATS)\n",
    "    )\n",
    "    terms_summary = parsed[\"terms_summary\"]\n",
    "    domains = parsed[\"domains\"]\n",
    "\n",
    "    if len(domains) != len(clauses):\n",
    "        domains = [\"K. Í∏∞ÌÉÄ Í≥ÑÏïΩ¬∑Î≥¥Ï¶ù\"] * len(clauses)\n",
    "\n",
    "    clause_results = [\n",
    "        process_clause(c, l, d)\n",
    "        for c, l, d in zip(clauses, labels, domains)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"terms_summary\": terms_summary,\n",
    "        \"clause_results\": clause_results\n",
    "    }\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 9. Îç∞Î™®\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    terms = Path(\"terms.txt\").read_text(encoding=\"utf-8\")\n",
    "    clauses_demo = [\n",
    "        \"Ï†ú22Ï°∞ (ÌôòÎ∂à) ÌöåÏÇ¨Îäî Ìï¥ÏßÄ Ïã†Ï≤≠ ÌõÑ 7Ïùº ÎÇ¥ ÌôòÎ∂àÌïúÎã§.\",\n",
    "        \"Ï†ú5Ï°∞ (Ï≤≠ÏïΩÏ≤†Ìöå) Í≥†Í∞ùÏùÄ ÏÉÅÌíà ÏàòÎ†π ÌõÑ 7Ïùº Ïù¥ÎÇ¥ Ï≤≠ÏïΩÏ≤†Ìöå Í∞ÄÎä•ÌïòÎã§.\"\n",
    "    ]\n",
    "    labels_demo = [\"Î∂àÎ¶¨\", \"Ïú†Î¶¨\"]\n",
    "\n",
    "    report = run_terms_analysis(terms, clauses_demo, labels_demo)\n",
    "\n",
    "    print(\"\\n‚óÜ ÏïΩÍ¥Ä ÏöîÏïΩ ‚óÜ\\n\", report[\"terms_summary\"])\n",
    "    for i, (r, lab) in enumerate(zip(report[\"clause_results\"], labels_demo), 1):\n",
    "        print(f\"\\n‚óá Ï°∞Ìï≠ {i} ({lab})\")\n",
    "        print(\"ÎèÑÎ©îÏù∏:\", r[\"domain\"])\n",
    "        print(r[\"llm_result\"])\n",
    "        if r[\"law_refs\"]:\n",
    "            print(\"Í¥ÄÎ†® Î≤ïÎ†π:\", \", \".join(r[\"law_refs\"])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8eb75",
   "metadata": {},
   "source": [
    "Î∞∞Ìè¨) ÌîÑÎ°†Ìä∏ ÏΩîÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import React, { useState } from 'react';\n",
    "import ReactMarkdown from 'react-markdown';\n",
    "import './App.css';\n",
    "\n",
    "// Í∞Å Î∂ÑÏÑù Í≤∞Í≥ºÎ•º ÌëúÏãúÌïòÍ∏∞ ÏúÑÌïú Î∂ÑÎ¶¨Îêú Ïª¥Ìè¨ÎÑåÌä∏Îì§\n",
    "\n",
    "// 1. TF-IDF Î∂ÑÏÑù Í≤∞Í≥º Ïª¥Ìè¨ÎÑåÌä∏ (ÏàòÏπò Ï†úÍ±∞)\n",
    "const TfidfResult = ({ result }) => (\n",
    "  <section className=\"result-section\">\n",
    "    <h2>üìö Ïñ¥Î†§Ïö¥ Îã®Ïñ¥ Î∞è Ïö©Ïñ¥ ÏÑ§Î™Ö</h2>\n",
    "    {result.difficult_words && result.difficult_words.length > 0 ? (\n",
    "      <ul>\n",
    "        {result.difficult_words.map((item, index) => (\n",
    "          <li key={index}>\n",
    "            {/* ÏöîÍµ¨ÏÇ¨Ìï≠ 4: TF-IDF Ï†êÏàò ÏàòÏπòÎ•º Ï†úÍ±∞Ìï©ÎãàÎã§. */}\n",
    "            <strong>{item.word}:</strong> {item.definition}\n",
    "          </li>\n",
    "        ))}\n",
    "      </ul>\n",
    "    ) : (\n",
    "      <p>ÌäπÎ≥ÑÌûà Ïñ¥Î†§Ïö¥ Îã®Ïñ¥Îäî Î∞úÍ≤¨ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.</p>\n",
    "    )}\n",
    "  </section>\n",
    ");\n",
    "\n",
    "// 2. SHAP/BERT Îã®Ïùº Ï°∞Ìï≠ Ïã¨Ï∏µ Î∂ÑÏÑù Í≤∞Í≥º Ïª¥Ìè¨ÎÑåÌä∏\n",
    "const ShapResult = ({ result }) => (\n",
    "  <section className=\"result-section\">\n",
    "    <h2>üí° AIÏùò ÏÉÅÏÑ∏ ÏÑ§Î™Ö Î∞è ÏÜîÎ£®ÏÖò</h2>\n",
    "    <div className=\"summary-box\">\n",
    "      <h3>üìå AI ÌåêÎã® ÏöîÏïΩ</h3>\n",
    "      <p>\n",
    "        AIÍ∞Ä Ïù¥ Ï°∞Ìï≠ÏùÑ <strong>'{result.prediction}'</strong>ÌïòÎã§Í≥† ÌåêÎã®ÌñàÏäµÎãàÎã§.<br/>\n",
    "        Ï£ºÏöî ÌåêÎã® Í∑ºÍ±∞Îäî <strong>\"{result.key_phrase}\"</strong> ÏôÄ(Í≥º) Í¥ÄÎ†®Îêú ÎÇ¥Ïö©ÏúºÎ°ú Î≥¥ÏûÖÎãàÎã§.\n",
    "      </p>\n",
    "    </div>\n",
    "    <div className=\"solution-box\">\n",
    "      <h3>üí¨ ÏÉÅÏÑ∏ Ìï¥ÏÑ§</h3>\n",
    "      <ReactMarkdown children={result.llm_explanation} />\n",
    "    </div>\n",
    "    {/* Í¥ÄÎ†® ÌÇ§ÏõåÎìú ÏÑπÏÖòÏùÄ TfidfResultÏôÄ Í≤πÏπòÎØÄÎ°ú Ïó¨Í∏∞ÏÑúÎäî ÏÉùÎûµÌïòÍ±∞ÎÇò Îã§Î•¥Í≤å ÌëúÌòÑÌï† Ïàò ÏûàÏäµÎãàÎã§. */}\n",
    "  </section>\n",
    ");\n",
    "\n",
    "// 3. Ï†ÑÏ≤¥ ÏïΩÍ¥Ä Top 3 ÌïÑÌÑ∞ÎßÅ Í≤∞Í≥º Ïª¥Ìè¨ÎÑåÌä∏ (ÏàòÏπò Ï†úÍ±∞)\n",
    "const Top3Result = ({ result }) => (\n",
    "  <section className=\"result-section\">\n",
    "    <h2>üìä Ï†ÑÏ≤¥ ÏïΩÍ¥Ä ÌïµÏã¨ Ï°∞Ìï≠ ÌïÑÌÑ∞ÎßÅ</h2>\n",
    "    <p>Ï¥ù {result.total_clauses_found}Í∞úÏùò Ï°∞Ìï≠ Ï§ëÏóêÏÑú Í∞ÄÏû• Ï£ºÎ™©Ìï† ÎßåÌïú Ï°∞Ìï≠Îì§ÏûÖÎãàÎã§.</p>\n",
    "    <div className=\"result-card danger\">\n",
    "      <h3>‚ùå Í∞ÄÏû• Î∂àÎ¶¨Ìïú Top 3 Ï°∞Ìï≠</h3>\n",
    "      {result.top_unfavorable_clauses.map((c, i) => (\n",
    "        <div key={`unfavorable-${i}`} className=\"clause-item\">\n",
    "          {/* ÏöîÍµ¨ÏÇ¨Ìï≠ 4: ÌôïÎ•†(%) ÏàòÏπòÎ•º Ï†úÍ±∞ÌïòÍ≥† Ï°∞Ìï≠ ÌÖçÏä§Ìä∏Îßå ÌëúÏãúÌï©ÎãàÎã§. */}\n",
    "          <p>{c.text}</p>\n",
    "        </div>\n",
    "      ))}\n",
    "    </div>\n",
    "    <div className=\"result-card safe\">\n",
    "      <h3>‚úÖ Í∞ÄÏû• Ïú†Î¶¨Ìïú Top 3 Ï°∞Ìï≠</h3>\n",
    "      {result.top_favorable_clauses.map((c, i) => (\n",
    "        <div key={`favorable-${i}`} className=\"clause-item\">\n",
    "          {/* ÏöîÍµ¨ÏÇ¨Ìï≠ 4: ÌôïÎ•†(%) ÏàòÏπòÎ•º Ï†úÍ±∞ÌïòÍ≥† Ï°∞Ìï≠ ÌÖçÏä§Ìä∏Îßå ÌëúÏãúÌï©ÎãàÎã§. */}\n",
    "          <p>{c.text}</p>\n",
    "        </div>\n",
    "      ))}\n",
    "    </div>\n",
    "  </section>\n",
    ");\n",
    "\n",
    "\n",
    "// Î©îÏù∏ Ïï± Ïª¥Ìè¨ÎÑåÌä∏\n",
    "function App() {\n",
    "  const [inputText, setInputText] = useState(\"\");\n",
    "  const [tfidfResult, setTfidfResult] = useState(null);\n",
    "  const [shapResult, setShapResult] = useState(null);\n",
    "  const [top3Result, setTop3Result] = useState(null);\n",
    "  const [isLoading, setIsLoading] = useState(false);\n",
    "  const [error, setError] = useState(null);\n",
    "\n",
    "  const handleComprehensiveAnalysis = async () => {\n",
    "    if (!inputText.trim()) {\n",
    "      setError(\"Î∂ÑÏÑùÌï† ÌÖçÏä§Ìä∏Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.\");\n",
    "      return;\n",
    "    }\n",
    "    \n",
    "    setIsLoading(true);\n",
    "    setError(null);\n",
    "    setTfidfResult(null);\n",
    "    setShapResult(null);\n",
    "    setTop3Result(null);\n",
    "\n",
    "    try {\n",
    "      // Promise.allÏùÑ ÏÇ¨Ïö©Ìï¥ 3Í∞úÏùò APIÎ•º ÎèôÏãúÏóê Ìò∏Ï∂ú\n",
    "      const [tfidfResponse, shapResponse, top3Response] = await Promise.all([\n",
    "        fetch(\"http://localhost:8080/extract-difficult-words\", {\n",
    "          method: \"POST\", headers: { \"Content-Type\": \"application/json\" }, body: JSON.stringify({ text: inputText }),\n",
    "        }),\n",
    "        fetch(\"http://localhost:8080/analyze-clause-with-shap\", {\n",
    "          method: \"POST\", headers: { \"Content-Type\": \"application/json\" }, body: JSON.stringify({ text: inputText }),\n",
    "        }),\n",
    "        fetch(\"http://localhost:8080/analyze-full-terms-top3\", {\n",
    "          method: \"POST\", headers: { \"Content-Type\": \"application/json\" }, body: JSON.stringify({ full_text: inputText }),\n",
    "        }),\n",
    "      ]);\n",
    "\n",
    "      if (!tfidfResponse.ok || !shapResponse.ok || !top3Response.ok) {\n",
    "        throw new Error(\"ÌïòÎÇò Ïù¥ÏÉÅÏùò Î∂ÑÏÑù APIÏóêÏÑú Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.\");\n",
    "      }\n",
    "\n",
    "      const tfidfData = await tfidfResponse.json();\n",
    "      const shapData = await shapResponse.json();\n",
    "      const top3Data = await top3Response.json();\n",
    "      \n",
    "      setTfidfResult(tfidfData);\n",
    "      setShapResult(shapData);\n",
    "      setTop3Result(top3Data);\n",
    "\n",
    "    } catch (err) {\n",
    "      setError(err.message);\n",
    "    } finally {\n",
    "      setIsLoading(false);\n",
    "    }\n",
    "  };\n",
    "\n",
    "  return (\n",
    "    <div className=\"container\">\n",
    "      <header>\n",
    "        <h1>üß† AI ÏïΩÍ¥Ä Ï¢ÖÌï© Î∂ÑÏÑù</h1>\n",
    "        <p>ÏûÖÎ†•Ìïú ÏïΩÍ¥ÄÏóê ÎåÄÌï¥ 3Í∞ÄÏßÄ AI Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Ìïú Î≤àÏóê Ï†úÍ≥µÌï©ÎãàÎã§.</p>\n",
    "      </header>\n",
    "\n",
    "      <section className=\"input-section\">\n",
    "        <textarea\n",
    "          rows=\"10\"\n",
    "          value={inputText}\n",
    "          onChange={(e) => setInputText(e.target.value)}\n",
    "          placeholder=\"Ïó¨Í∏∞Ïóê ÏïΩÍ¥Ä Ï°∞Ìï≠ ÎòêÎäî ÏïΩÍ¥Ä Ï†ÑÏ≤¥Î•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî...\"\n",
    "        />\n",
    "        <button onClick={handleComprehensiveAnalysis} disabled={isLoading}>\n",
    "          {isLoading ? \"üîÑ Î™®Îì† AI Î∂ÑÏÑù Ï§ë...\" : \"üîç Ï¢ÖÌï© Î∂ÑÏÑùÌïòÍ∏∞\"}\n",
    "        </button>\n",
    "      </section>\n",
    "      \n",
    "      {isLoading && <p className=\"loading-message\">AIÍ∞Ä ÏïΩÍ¥ÄÏùÑ Îã§Í∞ÅÎèÑÎ°ú Î∂ÑÏÑù Ï§ëÏûÖÎãàÎã§. Ïû†ÏãúÎßå Í∏∞Îã§Î†§Ï£ºÏÑ∏Ïöî...</p>}\n",
    "      {error && <p className=\"error-message\">Ïò§Î•ò: {error}</p>}\n",
    "\n",
    "      {/* Î™®Îì† Í≤∞Í≥ºÍ∞Ä ÎèÑÏ∞©ÌñàÏùÑ ÎïåÎßå, ÏöîÍµ¨ÏÇ¨Ìï≠Ïóê ÎßûÍ≤å Ïû¨Î∞∞ÏπòÎêú ÏàúÏÑúÎ°ú Î†åÎçîÎßÅ */}\n",
    "      {tfidfResult && shapResult && top3Result && (\n",
    "        <div className=\"all-results-container\">\n",
    "          {/* ÏöîÍµ¨ÏÇ¨Ìï≠ 1: Top3 Í≤∞Í≥ºÎ•º Í∞ÄÏû• Î®ºÏ†Ä ÌëúÏãú */}\n",
    "          <Top3Result result={top3Result} />\n",
    "          \n",
    "          {/* ÏöîÍµ¨ÏÇ¨Ìï≠ 2: AI ÏÉÅÏÑ∏ ÏÑ§Î™ÖÏùÑ Îëê Î≤àÏß∏Î°ú ÌëúÏãú */}\n",
    "          <ShapResult result={shapResult} />\n",
    "          \n",
    "          {/* ÏöîÍµ¨ÏÇ¨Ìï≠ 3: Îã®Ïñ¥ ÏÑ§Î™ÖÏùÑ ÎßàÏßÄÎßâÏúºÎ°ú ÌëúÏãú */}\n",
    "          <TfidfResult result={tfidfResult} />\n",
    "        </div>\n",
    "      )}\n",
    "    </div>\n",
    "  );\n",
    "}\n",
    "\n",
    "export default App;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674744a",
   "metadata": {},
   "source": [
    "Î∞∞Ìè¨) Î∞±ÏóîÎìú ÏΩîÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb79bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 1. ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏\n",
    "# ===================================================================\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import xml.etree.ElementTree as ET\n",
    "import shap\n",
    "from collections import defaultdict, Counter\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "import openai\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.schema.runnable import RunnableSequence\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ===================================================================\n",
    "# 2. Ï¥àÍ∏∞ ÏÑ§Ï†ï (FastAPI Ïï±, API ÌÇ§, Î™®Îç∏ Î°úÎìú)\n",
    "# ===================================================================\n",
    "app = FastAPI(\n",
    "    title=\"[ÏµúÏ¢Ö] ÌÜµÌï© ÏïΩÍ¥Ä Î∂ÑÏÑù API\",\n",
    "    description=\"Î™®Îì† Î∂ÑÏÑù Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌïòÎäî ÏµúÏ¢Ö Î≤ÑÏ†ÑÏûÖÎãàÎã§.\",\n",
    "    version=\"5.0.0\"\n",
    ")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\", \"http://127.0.0.1:3000\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = \"open_ai_key\"\n",
    "\n",
    "NAVER_CLIENT_ID = \"naver_api\"\n",
    "\n",
    "NAVER_CLIENT_SECRET = \"naver_api_key\"\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"C:/Users/xison/Desktop/ÌÖçÎßàÎ∞∞Ìè¨/ML/backend/model/domain_model\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.2, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# ===================================================================\n",
    "# 3. Î™®Îì† Í∏∞Îä•Î≥Ñ Î°úÏßÅ Î∞è Ìó¨Ìçº Ìï®Ïàò\n",
    "# ===================================================================\n",
    "\n",
    "# main.py ÌååÏùºÏùò Îã§Î•∏ Ìó¨Ìçº Ìï®ÏàòÎì§Í≥º Ìï®Íªò ÏïÑÎûò Ìï®ÏàòÎ•º Ï∂îÍ∞ÄÌï¥Ï£ºÏÑ∏Ïöî.\n",
    "\n",
    "def split_clauses(text: str) -> List[str]:\n",
    "    \"\"\"'Ï†ú nÏ°∞' Ìå®ÌÑ¥ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú ÌÖçÏä§Ìä∏ÏóêÏÑú Ï°∞Ìï≠Îì§ÏùÑ Î∂ÑÎ¶¨\"\"\"\n",
    "    # 'Ï†ú nÏ°∞(Ï†úÎ™©)' ÌòïÏãùÍπåÏßÄ Ìè¨Ìï®ÌïòÏó¨ Î∂ÑÎ¶¨ÌïòÎäî Ï†ïÍ∑úÏãù\n",
    "    pattern = re.compile(r'(Ï†ú\\s*\\d+\\s*Ï°∞\\s*(?:\\([^)]*\\))?[^\\n]*)')\n",
    "    \n",
    "    # Ï†ïÍ∑úÏãùÏúºÎ°ú ÌÖçÏä§Ìä∏Î•º Î∂ÑÎ¶¨\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    # Î∂ÑÎ¶¨Îêú Ï°∞Ìï≠Îì§ÏùÑ Ïû¨Ï°∞Ìï©\n",
    "    # parts Î¶¨Ïä§Ìä∏Îäî [Ï†ÑÎ¨∏, Ï†ú1Ï°∞, Ï†ú1Ï°∞ÎÇ¥Ïö©, Ï†ú2Ï°∞, Ï†ú2Ï°∞ÎÇ¥Ïö©, ...] ÌòïÏãùÏúºÎ°ú ÎÇòÎâ®\n",
    "    clauses = []\n",
    "    if len(parts) > 1:\n",
    "        # Ï≤´ Î≤àÏß∏ ÏöîÏÜåÎäî 'Ï†ú1Ï°∞' Ïù¥Ï†ÑÏùò ÏÑúÎ¨∏Ïù¥ÎØÄÎ°ú Ï†úÏô∏ÌïòÍ≥† ÏãúÏûë\n",
    "        for i in range(1, len(parts), 2):\n",
    "            if i + 1 < len(parts) and parts[i+1].strip():\n",
    "                # \"Ï†únÏ°∞(Ï†úÎ™©)\" + \"ÎÇ¥Ïö©\" ÏùÑ Ìï©Ï≥êÏÑú ÌïòÎÇòÏùò Ï°∞Ìï≠ÏúºÎ°ú ÎßåÎì¶\n",
    "                full_clause = parts[i].strip() + \"\\n\" + parts[i+1].strip()\n",
    "                clauses.append(full_clause)\n",
    "\n",
    "    # ÎßåÏïΩ 'Ï†ú nÏ°∞' Ìå®ÌÑ¥Ïù¥ ÌïòÎÇòÎèÑ ÏóÜÎã§Î©¥, Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏Î•º Îã®Ïùº Ï°∞Ìï≠ÏúºÎ°ú Í∞ÑÏ£º\n",
    "    if not clauses and text.strip():\n",
    "        return [text.strip()]\n",
    "        \n",
    "    return clauses\n",
    "\n",
    "# --- Í∏∞Îä• A: TF-IDF Î°úÏßÅ ---\n",
    "class TermsDifficultWordsExtractor:\n",
    "    def __init__(self, min_word_length: int = 2):\n",
    "        self.okt = Okt()\n",
    "        self.min_word_length = min_word_length\n",
    "        self.stopwords = {\"ÏùÄ\", \"Îäî\", \"Ïù¥\", \"Í∞Ä\", \"ÏùÑ\", \"Î•º\", \"Ïóê\", \"Ïùò\", \"ÏôÄ\", \"Í≥º\", \"ÎèÑ\", \"Î°ú\", \"ÏúºÎ°ú\", \"ÏóêÏÑú\", \"ÌöåÏÇ¨\", \"ÏÑúÎπÑÏä§\", \"ÌöåÏõê\", \"Ïù¥Ïö©\", \"ÏïΩÍ¥Ä\", \"Í≥†Í∞ù\", \"ÏÇ¨Ïù¥Ìä∏\", \"Î≥∏\", \"ÎãπÏÇ¨\", \"ÏÇ¨Ïö©\", \"Ï†ïÎ≥¥\", \"Í¥ÄÎ†®\", \"Ï†úÍ≥µ\", \"ÏúÑ\", \"Ïù¥Ìïò\", \"Í≤ΩÏö∞\", \"Îïå\", \"ÎÇ¥Ïö©\", \"Î™©Ï†Å\", \"Ï°∞Ìï≠\"}\n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        text = re.sub(r\"[^\\w\\sÍ∞Ä-Ìû£]\", \" \", text)\n",
    "        morphs = self.okt.pos(text, stem=True)\n",
    "        return [word for word, pos in morphs if pos == \"Noun\" and len(word) >= self.min_word_length and word not in self.stopwords and not word.isdigit()]\n",
    "    def load_corpus_from_file(self, path: str, lines_per_doc: int = 5) -> List[str]:\n",
    "        try:\n",
    "            with open(path, encoding=\"utf-8\") as f: lines = [ln.strip() for ln in f if ln.strip()]\n",
    "            return [\" \".join(lines[i : i + lines_per_doc]) for i in range(0, len(lines), lines_per_doc)]\n",
    "        except FileNotFoundError: return []\n",
    "    def extract_difficult_words(self, terms_text: str, daily_corpus_path: str, top_n: int = 5):\n",
    "        daily_raw = self.load_corpus_from_file(daily_corpus_path)\n",
    "        terms_tok = self.preprocess_text(terms_text)\n",
    "        daily_tok_list = [self.preprocess_text(doc) for doc in daily_raw]\n",
    "        docs = [\" \".join(terms_tok)] + [\" \".join(t) for t in daily_tok_list]\n",
    "        vec = TfidfVectorizer(token_pattern=r\"(?u)\\b[\\wÍ∞Ä-Ìû£]+\\b\", max_features=5000, min_df=1)\n",
    "        tfidf_mat = vec.fit_transform(docs)\n",
    "        feats = vec.get_feature_names_out()\n",
    "        terms_tfidf = tfidf_mat[0].toarray().flatten()\n",
    "        terms_cnt = Counter(terms_tok)\n",
    "        daily_cnt = Counter(tok for lst in daily_tok_list for tok in lst)\n",
    "        words = [{\"word\": w, \"tfidf\": terms_tfidf[i], \"terms_freq\": terms_cnt.get(w, 0), \"daily_freq\": daily_cnt.get(w, 0)} for i, w in enumerate(feats) if terms_cnt.get(w, 0) >= 1 and terms_tfidf[i] > 0 and daily_cnt.get(w, 0) < 10]\n",
    "\n",
    "        words = [word_data for word_data in words if len(word_data[\"word\"]) == 4]\n",
    "        words.sort(key=lambda x: x[\"tfidf\"], reverse=True)\n",
    "        return words[:top_n]\n",
    "\n",
    "\n",
    "tfidf_extractor = TermsDifficultWordsExtractor()\n",
    "\n",
    "# --- Í∏∞Îä• B, C, D Í≥µÏö© Ìó¨Ìçº Ìï®Ïàò ---\n",
    "def predict_clause_probabilities(text: str) -> dict:\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=1)[0].cpu().tolist()\n",
    "    return {\"prob_unfavorable\": probs[0], \"prob_favorable\": probs[1]}\n",
    "\n",
    "#ÎÑ§Ïù¥Î≤Ñ api Î∞±Í≥ºÏÇ¨Ï†Ñ Ìò∏Ï∂ú Í∏∞Îä•\n",
    "def search_naver_encyc(query: str):\n",
    "    encText = urllib.parse.quote(query)\n",
    "    url = f\"https://openapi.naver.com/v1/search/encyc.xml?query={encText}\"\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\", NAVER_CLIENT_ID)\n",
    "    request.add_header(\"X-Naver-Client-Secret\", NAVER_CLIENT_SECRET)\n",
    "    \n",
    "    try:\n",
    "        with urllib.request.urlopen(request, timeout=5) as response:\n",
    "            if response.getcode() == 200:\n",
    "                item = ET.fromstring(response.read()).find(\"channel/item\")\n",
    "                if item is not None and item.find(\"description\") is not None and item.find(\"description\").text:\n",
    "                    return re.sub('<.+?>', '', item.find(\"description\").text).strip()\n",
    "        return \"ÏÇ¨Ï†Ñ Í≤ÄÏÉâ Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§.\"\n",
    "    except Exception as e: return f\"ÏÇ¨Ï†Ñ API Ìò∏Ï∂ú Ï§ë Ïò§Î•ò({type(e).__name__}) Î∞úÏÉù\"\n",
    "\n",
    "# --- Í∏∞Îä• B: SHAP/BERT Í∞úÏÑ†Îêú Î∂ÑÏÑù Î°úÏßÅ ---\n",
    "class BertWrapper:\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model, self.tokenizer, self.device = model, tokenizer, device\n",
    "    def __call__(self, texts):\n",
    "        inputs = self.tokenizer(list(texts), return_tensors='pt', padding=True, truncation=True, max_length=128, add_special_tokens=True).to(self.device)\n",
    "        with torch.no_grad(): return F.softmax(self.model(**inputs).logits, dim=1).cpu().numpy()\n",
    "explainer = shap.Explainer(BertWrapper(model, tokenizer, device), shap.maskers.Text(tokenizer))\n",
    "\n",
    "def analyze_clause_with_shap(text: str):\n",
    "    probabilities = list(predict_clause_probabilities(text).values())\n",
    "    label_idx = torch.argmax(torch.tensor(probabilities)).item()\n",
    "    label_text = 'Ïú†Î¶¨' if label_idx == 1 else 'Î∂àÎ¶¨'\n",
    "    shap_values = explainer([text])\n",
    "    encoding = tokenizer(text, return_offsets_mapping=True, return_tensors='pt', truncation=True, max_length=128)\n",
    "    tokens, offsets, values = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]), encoding['offset_mapping'][0].tolist(), shap_values.values[0][:, label_idx]\n",
    "    words, scores, current_word, current_score, prev_end = [], [], '', 0.0, -1\n",
    "    for token, (start, end), score in zip(tokens, offsets, values):\n",
    "        if token in ['[CLS]', '[SEP]'] or start == end: continue\n",
    "        if start != prev_end and current_word:\n",
    "            words.append(current_word); scores.append(current_score); current_word, current_score = '', 0.0\n",
    "        current_word += text[start:end]; current_score += score; prev_end = end\n",
    "    if current_word: words.append(current_word); scores.append(current_score)\n",
    "    word_score_dict = defaultdict(float, {w.strip(): s for w, s in zip(words, scores)})\n",
    "    top_words = [w for w, _ in sorted(word_score_dict.items(), key=lambda x: abs(x[1]), reverse=True) if len(w)>1][:4]\n",
    "    sentence_words = re.findall(r'[Í∞Ä-Ìû£a-zA-Z0-9]+', text)\n",
    "    seen, phrases = set(), []\n",
    "    for i in range(len(sentence_words)):\n",
    "        for j in range(i + 1, min(len(sentence_words), i + 8) + 1):\n",
    "            chunk = ' '.join(sentence_words[i:j]).strip()\n",
    "            if chunk in seen or not any(w in chunk for w in top_words): continue\n",
    "            seen.add(chunk)\n",
    "            phrases.append((chunk, sum(word_score_dict.get(w, 0.0) for w in chunk.split() if w in word_score_dict)))\n",
    "    phrases.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    awkward_key_phrase = phrases[0][0] if phrases else \"ÌïµÏã¨ Íµ¨Î•º Ï∞æÏùÑ Ïàò ÏóÜÏùå\"\n",
    "    return {\"prediction\": label_text, \"awkward_key_phrase\": awkward_key_phrase, \"top_words\": top_words}\n",
    "\n",
    "# --- Í∏∞Îä• C: LangChain Î°úÏßÅ ---\n",
    "DOMAIN_CATS = [\"A. Í∏àÏúµÍ∏∞Í¥Ä\",\"B. Ï†ÑÏûêÏßÄÍ∏â¬∑ÌïÄÌÖåÌÅ¨\",\"C. Î≥¥Ìóò\",\"D. Ï¶ùÍ∂å¬∑Ìà¨Ïûê\",\"E. Ïú†ÌÜµ¬∑ÏÇ¨Ïù¥Î≤ÑÎ™∞\",\"F. ÌîÑÎûúÏ∞®Ïù¥Ï¶à¬∑Í≥µÍ∏â¬∑Î∂ÑÏñë¬∑Ïã†ÌÉÅ\",\"G. Î∂ÄÎèôÏÇ∞¬∑ÏûÑÎåÄÏ∞®¬∑Î¶¨Ïä§\",\"H. Ïö¥ÏÜ°¬∑Î¨ºÎ•ò\",\"I. Ïó¨Ìñâ¬∑Î†àÏ†Ä¬∑Í≤åÏûÑ\",\"J. ÏÉùÌôúÏÑúÎπÑÏä§\",\"K. Í∏∞ÌÉÄ Í≥ÑÏïΩ¬∑Î≥¥Ï¶ù\"]\n",
    "summary_schema, domain_schema = ResponseSchema(name=\"terms_summary\", description=\"ÏïΩÍ¥Ä Ï†ÑÎ¨∏ ÏöîÏïΩ\"), ResponseSchema(name=\"domains\", description=\"Ï°∞Ìï≠Î≥Ñ ÎèÑÎ©îÏù∏ Î¨∏ÏûêÏó¥ Î∞∞Ïó¥\")\n",
    "parser = StructuredOutputParser.from_response_schemas([summary_schema, domain_schema])\n",
    "prompt_template = ChatPromptTemplate.from_template(\"\"\"{format_instr}\\n\\nÏïΩÍ¥Ä Ï†ÑÎ¨∏:\\n{terms_text}\\n\\nÏ°∞Ìï≠ Î™©Î°ù:\\n{clauses}\\n\\nÏûëÏóÖ:\\n1) ÏïΩÍ¥Ä Ï†ÑÎ¨∏ÏùÑ ÏöîÏïΩÌïòÏÑ∏Ïöî.\\n2) Í∞Å Ï°∞Ìï≠Ïùò ÏÇ∞ÏóÖ Î∂ÑÏïºÎ•º ÏïÑÎûò Ïπ¥ÌÖåÍ≥†Î¶¨ Ï§ëÏóêÏÑú ÏÑ†ÌÉùÌïòÏó¨ ÎÇòÏó¥ÌïòÏÑ∏Ïöî.\\n\\nÏπ¥ÌÖåÍ≥†Î¶¨:\\n{domain_list}\"\"\")\n",
    "summary_chain = prompt_template | llm | parser\n",
    "\n",
    "def process_clause_with_langchain(clause: str, label: str, domain: str):\n",
    "    prompt_text = f\"\"\"[Ï°∞Ìï≠ ÏõêÎ¨∏]\n",
    "{clause}\n",
    "\n",
    "[Î∂ÑÏÑù ÏûëÏóÖ]\n",
    "ÎãπÏã†ÏùÄ ÏÜåÎπÑÏûêÏùò ÏûÖÏû•ÏóêÏÑú ÏïΩÍ¥ÄÏùÑ Î∂ÑÏÑùÌïòÎäî AI Î≤ïÎ•† Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§.\n",
    "1. ÏúÑ Ï°∞Ìï≠Ïù¥ Ïôú ÏÜåÎπÑÏûêÏóêÍ≤å '{label}'ÌïúÏßÄ Î™ÖÌôïÌïòÍ≥† Ïù¥Ìï¥ÌïòÍ∏∞ ÏâΩÍ≤å 2~3Î¨∏Ïû•ÏúºÎ°ú ÏÑ§Î™ÖÌïòÏÑ∏Ïöî.\n",
    "2. {'Î∂àÎ¶¨ Ï°∞Ìï≠ÏùÑ ÏÜåÎπÑÏûêÏóêÍ≤å Ïú†Î¶¨ÌïòÍ≤å Í∞úÏ†ïÌïòÎäî ÏïàÏùÑ Ï†úÏãúÌïòÏÑ∏Ïöî. \"Í∞úÏ†ï Ï†úÏïà:\" Ïù¥ÎùºÎäî Ï†úÎ™©ÏúºÎ°ú ÏãúÏûëÌï¥Ï£ºÏÑ∏Ïöî.' if label == \"Î∂àÎ¶¨\" else 'Ìï¥Îãπ Ï°∞Ìï≠ÏùÄ ÏÜåÎπÑÏûêÏóêÍ≤å Ïú†Î¶¨ÌïòÏßÄÎßå, Îçî Í∞úÏÑ†Ìï† Î∂ÄÎ∂ÑÏù¥ ÏûàÎã§Î©¥ Ï†úÏïàÌï¥Ï£ºÏÑ∏Ïöî. \"Í∞úÏÑ† Ï†úÏïà:\" Ïù¥ÎùºÎäî Ï†úÎ™©ÏúºÎ°ú ÏãúÏûëÌï¥Ï£ºÏÑ∏Ïöî.'}\n",
    "3. Ïù¥ Ï°∞Ìï≠Í≥º Í¥ÄÎ†®Îêú ÏùºÎ∞òÏ†ÅÏù∏ Î≤ïÎ•†(Ïòà: ÏïΩÍ¥ÄÍ∑úÏ†úÎ≤ï, Ï†ÑÏûêÏÉÅÍ±∞ÎûòÎ≤ï Îì±)Ïù¥ ÏûàÎã§Î©¥ Ïñ¥Îñ§ Í≤ÉÏù∏ÏßÄ Ïñ∏Í∏âÌï¥Ï£ºÏÑ∏Ïöî.\"\"\"\n",
    "    llm_result = llm.predict(prompt_text).strip()\n",
    "    return {\"domain\": domain, \"original_clause\": clause, \"label\": label, \"analysis_and_suggestion\": llm_result}\n",
    "\n",
    "\n",
    "def run_terms_analysis(terms_text: str, clauses: List[str], labels: List[str]):\n",
    "    try:\n",
    "        parsed = summary_chain.invoke({\"terms_text\": terms_text, \"clauses\": \"\\n\".join(f\"{i+1}) {c}\" for i, c in enumerate(clauses)), \"format_instr\": parser.get_format_instructions(), \"domain_list\": \"\\n\".join(DOMAIN_CATS)})\n",
    "        terms_summary, domains = parsed[\"terms_summary\"], parsed[\"domains\"]\n",
    "    except Exception as e:\n",
    "        terms_summary, domains = f\"ÏöîÏïΩ/ÎèÑÎ©îÏù∏ Î∂ÑÎ•ò Ïã§Ìå®: {e}\", [\"K. Í∏∞ÌÉÄ Í≥ÑÏïΩ¬∑Î≥¥Ï¶ù\"] * len(clauses)\n",
    "    if len(domains) != len(clauses): domains = [\"K. Í∏∞ÌÉÄ Í≥ÑÏïΩ¬∑Î≥¥Ï¶ù\"] * len(clauses)\n",
    "    return {\"terms_summary\": terms_summary, \"clause_results\": [process_clause_with_langchain(c, l, d) for c, l, d in zip(clauses, labels, domains)]}\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 5. FastAPI Pydantic Î™®Îç∏ Î∞è ÏóîÎìúÌè¨Ïù∏Ìä∏\n",
    "# ===================================================================\n",
    "\n",
    "# --- ÏûÖÎ†• Î™®Îç∏ ---\n",
    "class TextIn(BaseModel): text: str\n",
    "class FullTermsIn(BaseModel): full_text: str\n",
    "class LangchainIn(BaseModel): terms_text: str; clauses: List[str]; labels: List[str]\n",
    "\n",
    "# --- ÏóîÎìúÌè¨Ïù∏Ìä∏ ---\n",
    "\n",
    "@app.post(\"/extract-difficult-words\", summary=\"[Í∏∞Îä• 1] TF-IDF Ïñ¥Î†§Ïö¥ Îã®Ïñ¥ Ï∂îÏ∂ú (Í∏∞Ï°¥ Í∏∞Îä•)\")\n",
    "async def endpoint_extract_difficult_words(payload: TextIn):\n",
    "    try:\n",
    "        difficult_words = tfidf_extractor.extract_difficult_words(payload.text, \"corpus.txt\", top_n=5)\n",
    "        for word_data in difficult_words: word_data[\"definition\"] = search_naver_encyc(word_data[\"word\"])\n",
    "        return {\"difficult_words\": difficult_words}\n",
    "    except Exception as e: raise HTTPException(500, f\"TF-IDF Î∂ÑÏÑù Ïò§Î•ò: {e}\")\n",
    "\n",
    "@app.post(\"/analyze-clause-with-shap\", summary=\"[Í∏∞Îä• 2] Îã®Ïùº Ï°∞Ìï≠ Ïã¨Ï∏µ Î∂ÑÏÑù (Í∞úÏÑ†Îê®)\")\n",
    "async def endpoint_analyze_clause_with_shap(payload: TextIn):\n",
    "    analysis = analyze_clause_with_shap(payload.text)\n",
    "    natural_phrase_prompt = f\"Îã§Ïùå Î¨∏Ïû•ÏùÄ Ïª¥Ìì®ÌÑ∞ Î∂ÑÏÑù Í≤∞Í≥ºÎ°ú Ïñ¥ÏÉâÌï©ÎãàÎã§. ÌïµÏã¨ ÏùòÎØ∏Îäî Ïú†ÏßÄÌïòÎêò, ÏûêÏó∞Ïä§Îü¨Ïö¥ ÌïúÍµ≠Ïñ¥ Ìïú Î¨∏Ïû•ÏúºÎ°ú Îã§Îì¨Ïñ¥ Ï£ºÏÑ∏Ïöî: '{analysis['awkward_key_phrase']}'\"\n",
    "    natural_key_phrase = llm.predict(natural_phrase_prompt)\n",
    "    explanation_prompt = f\"\"\"[Î∂ÑÏÑù ÎåÄÏÉÅ Ï°∞Ìï≠]\\n\"{payload.text}\"\\n\\n[AI Î∂ÑÏÑù Í≤∞Í≥º]\\n- Ïù¥ Ï°∞Ìï≠ÏùÄ '{analysis['prediction']}'ÌïòÎã§Í≥† ÌåêÎã®Îê©ÎãàÎã§.\\n- ÌåêÎã® Í∑ºÍ±∞Îäî '{natural_key_phrase}' ÎÇ¥Ïö©ÏûÖÎãàÎã§.\\n\\n[ÏöîÏ≤≠ ÏûëÏóÖ]\\nAI Î≤ïÎ•† Ï†ÑÎ¨∏Í∞ÄÎ°úÏÑú, ÏúÑ Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú ÏïÑÎûò Ìï≠Î™©Ïóê ÎåÄÌï¥ Markdown ÌòïÏãùÏúºÎ°ú ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî.\\n\\n- **ÌïµÏã¨ Î¨∏Ï†úÏ†ê/ÌòúÌÉù**:\\n- **ÏÜåÎπÑÏûêÏóêÍ≤å ÎØ∏ÏπòÎäî ÏòÅÌñ•**:\\n- **Í∂åÏû• ÎåÄÏùë Î∞©Ïïà**:\"\"\"\n",
    "    llm_explanation = llm.predict(explanation_prompt)\n",
    "    definitions = [{\"word\": w, \"definition\": search_naver_encyc(w)} for w in analysis['top_words']]\n",
    "    return {\"prediction\": analysis['prediction'], \"key_phrase\": natural_key_phrase, \"llm_explanation\": llm_explanation, \"keywords_definitions\": definitions}\n",
    "\n",
    "@app.post(\"/analyze-terms-with-langchain\", summary=\"[Í∏∞Îä• 3] LangChain Ï†ÑÏ≤¥ ÏïΩÍ¥Ä Î∂ÑÏÑù (Í∏∞Ï°¥ Í∏∞Îä•)\")\n",
    "async def endpoint_analyze_terms_with_langchain(payload: LangchainIn):\n",
    "    if len(payload.clauses) != len(payload.labels): raise HTTPException(400, \"clausesÏôÄ labels Í∞úÏàò Î∂àÏùºÏπò\")\n",
    "    try: return run_terms_analysis(payload.terms_text, payload.clauses, payload.labels)\n",
    "    except Exception as e: raise HTTPException(500, f\"LangChain Î∂ÑÏÑù Ïò§Î•ò: {e}\")\n",
    "\n",
    "@app.post(\"/analyze-full-terms-top3\", summary=\"[Í∏∞Îä• 4] Ï†ÑÏ≤¥ ÏïΩÍ¥Ä Top 3 ÌïÑÌÑ∞ÎßÅ (Ïã†Í∑ú Ï∂îÍ∞Ä)\")\n",
    "async def endpoint_analyze_full_terms_top3(payload: FullTermsIn):\n",
    "    clauses = split_clauses(payload.full_text)\n",
    "    if not clauses: raise HTTPException(400, \"Î∂ÑÏÑùÌï† Ï°∞Ìï≠ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå\")\n",
    "    all_results = [dict(text=c, **predict_clause_probabilities(c)) for c in clauses]\n",
    "    top_favorable = sorted(all_results, key=lambda x: x['prob_favorable'], reverse=True)[:3]\n",
    "    top_unfavorable = sorted(all_results, key=lambda x: x['prob_unfavorable'], reverse=True)[:3]\n",
    "    return {\"total_clauses_found\": len(clauses), \"top_favorable_clauses\": top_favorable, \"top_unfavorable_clauses\": top_unfavorable}\n",
    "\n",
    "@app.get(\"/\", summary=\"ÏÑúÎ≤Ñ ÏÉÅÌÉú ÌôïÏù∏\")\n",
    "def read_root(): return {\"status\": \"ÏïΩÍ¥Ä Î∂ÑÏÑù API ÏÑúÎ≤Ñ v5.0 (Î™®Îì† Í∏∞Îä• ÌÜµÌï©)Ïù¥ Ï†ïÏÉÅ ÎèôÏûë Ï§ëÏûÖÎãàÎã§.\"} "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
