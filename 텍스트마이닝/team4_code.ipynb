{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c54aba26",
   "metadata": {},
   "source": [
    "ëª¨ë¸ í•™ìŠµì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f8119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "\n",
    "# ================================\n",
    "# âœ… í™˜ê²½ ì„¤ì •\n",
    "# ================================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"kykim/bert-kor-base\")  # KorSci-BERT\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"kykim/bert-kor-base\",\n",
    "    num_labels=2,\n",
    "    use_safetensors=True  # ë³´ì•ˆ ì´ìŠˆ ìš°íšŒ\n",
    ").to(device)\n",
    "\n",
    "# ================================\n",
    "# âœ… ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "# ================================\n",
    "train_dataset = ConcatDataset([\n",
    "    ClauseDataset('T_P', tokenizer, 1),\n",
    "    ClauseDataset('T_N', tokenizer, 0)\n",
    "])\n",
    "val_dataset = ConcatDataset([\n",
    "    ClauseDataset('V_P', tokenizer, 1),\n",
    "    ClauseDataset('V_N', tokenizer, 0)\n",
    "])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# ================================\n",
    "# âœ… í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì • Loss ì„¤ì •\n",
    "# ================================\n",
    "pos_count = 5932\n",
    "neg_count = 3172\n",
    "total = pos_count + neg_count\n",
    "\n",
    "# ë¶ˆë¦¬(label=0), ìœ ë¦¬(label=1)\n",
    "class_weights = torch.tensor([total / neg_count, total / pos_count], dtype=torch.float32).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# ================================\n",
    "# âœ… í•™ìŠµ ë£¨í”„ í•¨ìˆ˜\n",
    "# ================================\n",
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Train\"):\n",
    "        input_ids, attn_mask, labels = [x.to(device) for x in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "# ================================\n",
    "# âœ… ê²€ì¦ ë£¨í”„ í•¨ìˆ˜\n",
    "# ================================\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Val\"):\n",
    "            input_ids, attn_mask, labels = [x.to(device) for x in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"ë¶ˆë¦¬\", \"ìœ ë¦¬\"], digits=4)\n",
    "    return total_loss / len(dataloader), correct / total, report\n",
    "\n",
    "# ================================\n",
    "# âœ… í•™ìŠµ ì‹¤í–‰ (ìµœê³  ëª¨ë¸ ì €ì¥ í¬í•¨)\n",
    "# ================================\n",
    "best_acc = 0.0\n",
    "early_stop_count = 0\n",
    "EARLY_STOP = 3\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\nğŸ” Epoch {epoch}\")\n",
    "    \n",
    "    # ğŸ”¹ í•™ìŠµ\n",
    "    train_loss, train_acc = train(model, train_loader)\n",
    "    \n",
    "    # ğŸ”¹ ê²€ì¦\n",
    "    val_loss, val_acc, val_report = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"ğŸ“˜ Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n",
    "    print(f\"ğŸ“— Val Loss:   {val_loss:.4f} | Acc: {val_acc:.4f}\")\n",
    "    print(f\"\\n{val_report}\")\n",
    "\n",
    "    # âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_korsci_bert_model.pt\")\n",
    "        print(\"âœ… Best model saved.\")\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "        print(f\"â¸ No improvement. Early stop count: {early_stop_count}\")\n",
    "\n",
    "    # â›”ï¸ ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´\n",
    "    if early_stop_count >= EARLY_STOP:\n",
    "        print(\"â›”ï¸ Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06168a4",
   "metadata": {},
   "source": [
    "SHAP ë¶„ì„ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e06802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "\n",
    "# âœ… ëª¨ë¸ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"./domain_model\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# âœ… stopwords ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(\"stopwords_ko.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    STOPWORDS = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "# âœ… ìœ í‹¸ í•¨ìˆ˜\n",
    "def is_structural_phrase(word):\n",
    "    return re.match(r'^ì œ\\d+í•­$', word) or re.match(r'^ì œ\\d+ì¡°$', word) or re.match(r'^ì œ\\d+$', word) or re.match(r'^\\d+$', word) or word in {'â‘ ','â‘¡','â‘¢','â‘£','â‘¤','â‘¥','â‘¦'}\n",
    "\n",
    "def remove_clause_title(text):\n",
    "    lines = text.strip().split(\"\\n\")\n",
    "    return \"\\n\".join(lines[1:]).strip() if re.match(r'^ì œ\\d+ì¡°', lines[0]) else text\n",
    "\n",
    "def load_all_clauses(folders):\n",
    "    clauses = []\n",
    "    for folder in folders:\n",
    "        for fname in os.listdir(folder):\n",
    "            if fname.endswith(\".json\"):\n",
    "                with open(os.path.join(folder, fname), 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    clauses.extend(clause.strip() for clause in data.get(\"clauseArticle\", []) if isinstance(clause, str))\n",
    "    return clauses\n",
    "\n",
    "# âœ… SHAPìš© ë˜í¼\n",
    "class BertWrapper:\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "    def __call__(self, texts):\n",
    "        inputs = self.tokenizer(list(texts), return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        input_ids = inputs['input_ids'].to(self.device)\n",
    "        attention_mask = inputs['attention_mask'].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "# âœ… SHAP ë‹¨ì–´ ë³‘í•©\n",
    "def merge_tokens_by_offset(text, shap_values, tokenizer, label_idx):\n",
    "    encoding = tokenizer(text, return_offsets_mapping=True, return_tensors='pt', truncation=True, max_length=128)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
    "    offsets = encoding['offset_mapping'][0].tolist()\n",
    "    values = shap_values.values[0][:, label_idx]\n",
    "    words, scores, current_word, current_score, prev_end = [], [], '', 0.0, -1\n",
    "\n",
    "    for token, (start, end), score in zip(tokens, offsets, values):\n",
    "        if token in ['[CLS]', '[SEP]'] or start == end:\n",
    "            continue\n",
    "        if start != prev_end and current_word:\n",
    "            words.append(current_word)\n",
    "            scores.append(current_score)\n",
    "            current_word, current_score = '', 0.0\n",
    "        current_word += text[start:end]\n",
    "        current_score += score\n",
    "        prev_end = end\n",
    "\n",
    "    if current_word:\n",
    "        words.append(current_word)\n",
    "        scores.append(current_score)\n",
    "\n",
    "    filtered = [(w.strip(), s) for w, s in zip(words, scores)\n",
    "                if len(w) > 1 and not re.match(r'^[\\W\\d]+$', w) and w not in STOPWORDS and not is_structural_phrase(w)]\n",
    "    return filtered\n",
    "\n",
    "# âœ… í•µì‹¬ êµ¬ ì¶”ì¶œ\n",
    "def extract_phrases_with_scores(sentence, top_words, word_score_dict, window=8):\n",
    "    words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', sentence)\n",
    "    seen, phrases = set(), []\n",
    "    for i in range(len(words)):\n",
    "        for j in range(i+1, min(len(words), i+window)+1):\n",
    "            chunk = ' '.join(words[i:j]).strip()\n",
    "            if chunk in seen: continue\n",
    "            seen.add(chunk)\n",
    "            matched = [w for w in top_words if w in chunk]\n",
    "            if len(matched) >= 1:\n",
    "                score_sum = sum(word_score_dict.get(w, 0.0) for w in matched)\n",
    "                phrases.append((chunk, score_sum))\n",
    "    phrases.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    return phrases[:1]\n",
    "\n",
    "# âœ… ë³µí•©ëª…ì‚¬ ê¸°ë°˜ n-gram\n",
    "def compute_ngram_shap_sum(nouns, word_score_dict, label_text, n=3):\n",
    "    def contains_stopword(phrase): return any(sw in phrase for sw in {\"ì œ\",\"ì¡°\",\"í•­\",\"í˜¸\",\"ë²ˆí˜¸\"}) or re.search(r'[^\\wê°€-í£ ]', phrase)\n",
    "    filtered = [n for n in nouns if not contains_stopword(n) and not is_structural_phrase(n)]\n",
    "    ngram_score_sum = {}\n",
    "    for i in range(len(filtered) - n + 1):\n",
    "        ngram = ' '.join(filtered[i:i+n])\n",
    "        if ngram in ngram_score_sum: continue\n",
    "        score_sum = sum(word_score_dict.get(w, 0.0) for w in filtered[i:i+n])\n",
    "        if not contains_stopword(ngram) and score_sum > 0:\n",
    "            ngram_score_sum[ngram] = score_sum\n",
    "    results = sorted(ngram_score_sum.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    seen, final = set(), []\n",
    "    for phrase, score in results:\n",
    "        if any(p in seen for p in phrase.split()): continue\n",
    "        final.append((phrase, score))\n",
    "        seen.update(phrase.split())\n",
    "        if len(final) >= 3: break\n",
    "    print(f\"\\nğŸ“Œ SHAP ê¸°ì¤€, '{label_text}' íŒë‹¨ì— ê°€ì¥ í° ì˜í–¥ì„ ì¤€ {n}-gram í‘œí˜„ (ë³µí•©ëª…ì‚¬ ê¸°ë°˜):\")\n",
    "    for phrase, score in final:\n",
    "        print(f\"  - '{phrase}': SUM = {score:.4f}\")\n",
    "\n",
    "# âœ… ë³µí•©ëª…ì‚¬ ì¶”ì¶œê¸° í•™ìŠµ\n",
    "all_folders = [\"C:/data/TL_ìœ ë¦¬\", \"C:/data/TL_ë¶ˆë¦¬\", \"C:/data/VL_ìœ ë¦¬\", \"C:/data/VL_ë¶ˆë¦¬\"]\n",
    "corpus = load_all_clauses(all_folders)\n",
    "with open(\"processed_clauses.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus += [line.strip() for line in f if line.strip()]\n",
    "print(f\"âœ… ì „ì²´ í•™ìŠµ ë¬¸ì¥ ìˆ˜: {len(corpus)}\")\n",
    "noun_extractor = LRNounExtractor_v2(verbose=False)\n",
    "noun_extractor.train(corpus)\n",
    "nouns_score_dict = noun_extractor.extract()\n",
    "\n",
    "# âœ… ì…ë ¥ ë¬¸ì¥ ì§ì ‘ ì§€ì •\n",
    "input_text = \"â‘¡ ì´ ê²½ìš° íšŒì‚¬ê°€ íšŒì›ì—ê²Œ í™˜ê¸‰ì„ ì§€ì—°í•œ ë•Œì—ëŠ” ê·¸ ì§€ì—°ê¸°ê°„ì— ëŒ€í•˜ì—¬ ì „ììƒê±°ë˜ ë“±ì—ì„œì˜ ì†Œë¹„ìë³´í˜¸ì— ê´€í•œ ë²•ë¥  ë° ì‹œí–‰ë ¹ì—ì„œ ì •í•˜ëŠ” ì´ìœ¨ì„ ê³±í•˜ì—¬ ì‚°ì •í•œ ì§€ì—°ì´ìë¥¼ ì§€ê¸‰í•´ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "sentence = remove_clause_title(input_text)\n",
    "\n",
    "# âœ… ì˜ˆì¸¡ + í•´ì„\n",
    "wrapper = BertWrapper(model, tokenizer, device)\n",
    "probs = wrapper([sentence])[0]\n",
    "label = int(probs.argmax())\n",
    "label_text = 'ìœ ë¦¬' if label == 1 else 'ë¶ˆë¦¬'\n",
    "print(f\"\\nâœ… ì˜ˆì¸¡ ê²°ê³¼: {label_text} ({probs[label]:.4f})\")\n",
    "\n",
    "explainer = shap.Explainer(wrapper, shap.maskers.Text(tokenizer))\n",
    "shap_values = explainer([sentence])\n",
    "shap.plots.text(shap_values[0])\n",
    "\n",
    "merged = merge_tokens_by_offset(sentence, shap_values, tokenizer, label)\n",
    "word_score_dict = defaultdict(float)\n",
    "for w, s in merged:\n",
    "    word_score_dict[w] += s\n",
    "\n",
    "top_words = [w for w, _ in sorted(word_score_dict.items(), key=lambda x: abs(x[1]), reverse=True)[:10]]\n",
    "print(f\"\\nğŸ” SHAP ê¸°ì¤€ ìƒìœ„ ë‹¨ì–´: {top_words}\")\n",
    "\n",
    "phrase = extract_phrases_with_scores(sentence, top_words, word_score_dict)\n",
    "print(f\"\\nğŸ“Œ {label_text} íŒë‹¨ í•µì‹¬ êµ¬(Phrase):\")\n",
    "for p, s in phrase:\n",
    "    print(f\"  - {p}: SUM = {s:.4f}\")\n",
    "\n",
    "nouns = [n for n in nouns_score_dict if n in sentence and not is_structural_phrase(n)]\n",
    "compute_ngram_shap_sum(nouns, word_score_dict, label_text=label_text, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5893d01-764c-422a-bb83-6c676b5e0799",
   "metadata": {},
   "source": [
    "ì•½ê´€ ì¡°í•­ ì „ì²˜ë¦¬ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f06044-cb51-4ebd-9d13-b81070749614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. í•„ìš” ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# 3. ì¡°í•­ ë¶„ë¦¬ í•¨ìˆ˜ (ê°„ë‹¨íˆ 'ì œ n ì¡°' ê¸°ì¤€ ë¶„ë¦¬)\n",
    "def split_clauses(text):\n",
    "    pattern = re.compile(r'(ì œ\\s*\\d+\\s*ì¡°[^\\n]*)')\n",
    "    parts = pattern.split(text)\n",
    "    clauses = []\n",
    "    for i in range(1, len(parts), 2):\n",
    "        title = parts[i].strip()\n",
    "        content = parts[i+1].strip() if i+1 < len(parts) else ''\n",
    "        full_text = title + \" \" + content\n",
    "        clauses.append(full_text)\n",
    "    return clauses\n",
    "\n",
    "# 4. ëª¨ë¸ ê²½ë¡œ ì§€ì • (ìì‹ ì˜ ê²½ë¡œë¡œ ìˆ˜ì •)\n",
    "MODEL_PATH = \"/content/drive/MyDrive/domain_model\"\n",
    "\n",
    "# 5. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 6. ì¡°í•­ë³„ ì˜ˆì¸¡ í•¨ìˆ˜ (softmax í™•ë¥  + ì˜ˆì¸¡ ë¼ë²¨ ë°˜í™˜)\n",
    "def predict_with_probs(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred_label = torch.argmax(probs, dim=1).item()\n",
    "        prob_vals = probs[0].cpu().tolist()\n",
    "    return pred_label, prob_vals\n",
    "\n",
    "# 7. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ì—¬ëŸ¬ íŒŒì¼ ì²˜ë¦¬)\n",
    "def main():\n",
    "    # âœ… ë¶„ì„í•  ì•½ê´€ íŒŒì¼ ê²½ë¡œë“¤ (ìì‹ ì˜ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ìˆ˜ì •)\n",
    "    terms_paths = [\n",
    "        '/content/drive/MyDrive/001_ê°œì¸ì •ë³´ì·¨ê¸‰ë°©ì¹¨_ê°€ê³µ.xml',\n",
    "        '/content/drive/MyDrive/001_ê°€ë§¹ê³„ì•½_ê°€ê³µ.xml',\n",
    "        '/content/drive/MyDrive/001_ê²°í˜¼ì •ë³´ì„œë¹„ìŠ¤_ê°€ê³µ.xml',\n",
    "        '/content/drive/MyDrive/001_ê³µê¸‰ê³„ì•½_ê°€ê³µ.xml'\n",
    "    ]\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    for path in terms_paths:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            full_text = f.read()\n",
    "\n",
    "        clauses = split_clauses(full_text)\n",
    "        results = []\n",
    "\n",
    "        for clause in clauses:\n",
    "            label, probs = predict_with_probs(clause)\n",
    "            results.append({\n",
    "                \"text\": clause,\n",
    "                \"label\": label,\n",
    "                \"prob_unfavorable\": probs[0],  # ë¶ˆë¦¬ í™•ë¥ \n",
    "                \"prob_favorable\": probs[1]    # ìœ ë¦¬ í™•ë¥ \n",
    "            })\n",
    "\n",
    "        filename = os.path.basename(path)\n",
    "        all_results[filename] = results\n",
    "\n",
    "        # ğŸ” Top ì¡°í•­ ì¶œë ¥\n",
    "        top_favorable = sorted(results, key=lambda x: x['prob_favorable'], reverse=True)[:3]\n",
    "        top_unfavorable = sorted(results, key=lambda x: x['prob_unfavorable'], reverse=True)[:3]\n",
    "\n",
    "        print(f\"\\nğŸ“„ {filename} ê²°ê³¼:\")\n",
    "        print(\"=== âœ… ìœ ë¦¬ í™•ë¥  Top 3 ===\")\n",
    "        for i, r in enumerate(top_favorable, 1):\n",
    "            print(f\"{i}. í™•ë¥ : {r['prob_favorable']:.4f} | í…ìŠ¤íŠ¸: {r['text'][:100]}...\")\n",
    "\n",
    "        print(\"=== âš ï¸ ë¶ˆë¦¬ í™•ë¥  Top 3 ===\")\n",
    "        for i, r in enumerate(top_unfavorable, 1):\n",
    "            print(f\"{i}. í™•ë¥ : {r['prob_unfavorable']:.4f} | í…ìŠ¤íŠ¸: {r['text'][:100]}...\")\n",
    "\n",
    "    # ğŸ“ ê²°ê³¼ JSON ì €ì¥\n",
    "    output_path = '/content/drive/MyDrive/clause_predictions_all.json'\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\nâœ… ì „ì²´ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "\n",
    "# 8. ì‹¤í–‰\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e31458",
   "metadata": {},
   "source": [
    "TF-IDF ë‹¨ì–´ ì¶”ì¶œ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b2116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "from collections import Counter\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LOGGING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "logging.basicConfig(\n",
    "    filename=\"terms_extractor_debug.log\",\n",
    "    filemode=\"w\",\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TermsDifficultWordsExtractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        min_word_length: int = 2,\n",
    "        allow_single_char_noun: bool = True,\n",
    "        debug: bool = True,\n",
    "    ):\n",
    "        self.okt = Okt()\n",
    "        self.min_word_length = min_word_length\n",
    "        self.allow_single_char_noun = allow_single_char_noun\n",
    "        self.debug = debug\n",
    "\n",
    "        # â”€â”€ ê¸°ë³¸+ì•½ì‹ TOS ë¶ˆìš©ì–´ â”€â”€\n",
    "        self.stopwords = {\n",
    "            \"ì€\", \"ëŠ”\", \"ì´\", \"ê°€\", \"ì„\", \"ë¥¼\", \"ì—\", \"ì˜\", \"ì™€\", \"ê³¼\",\n",
    "            \"ë„\", \"ë¡œ\", \"ìœ¼ë¡œ\", \"ì—ì„œ\", \"ê¹Œì§€\", \"ë¶€í„°\", \"ë§Œ\", \"ë¼ë„\",\n",
    "            \"ì¡°ì°¨\", \"ë§ˆì €\", \"ì—ê²Œ\", \"í•œí…Œ\", \"ë°\", \"ë˜ëŠ”\", \"ê·¸ë¦¬ê³ \",\n",
    "            \"í•˜ì§€ë§Œ\", \"ê·¸ëŸ¬ë‚˜\", \"ë”°ë¼ì„œ\",\n",
    "            \"íšŒì‚¬\", \"ì„œë¹„ìŠ¤\", \"íšŒì›\", \"ì´ìš©\", \"ì•½ê´€\", \"ê³ ê°\", \"ì‚¬ì´íŠ¸\",\n",
    "            \"ì›¹ì‚¬ì´íŠ¸\", \"ë³¸\", \"ë‹¹ì‚¬\", \"ì‚¬ìš©\", \"ì •ë³´\", \"ê´€ë ¨\", \"ì œê³µ\",\n",
    "            \"ìœ„\", \"ì´í•˜\", \"ê²½ìš°\", \"ë•Œ\", \"ë‚´ìš©\", \"ëª©ì \", \"ì¡°í•­\", \"ì œê³µì\",\n",
    "            \"íšŒì›ì›\",\n",
    "            \"í•©ë‹ˆë‹¤\", \"í•©ë‹ˆë‹¤.\",\n",
    "        }\n",
    "\n",
    "    # â”€â”€ ë‚´ë¶€ ë””ë²„ê·¸ â”€â”€\n",
    "    def _dbg(self, msg: str) -> None:\n",
    "        if self.debug:\n",
    "            log.debug(msg)\n",
    "\n",
    "    # â”€â”€ ì „ì²˜ë¦¬: ëª…ì‚¬ë§Œ ë‚¨ê¸°ê¸° â”€â”€\n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        text = re.sub(r\"[^\\w\\sê°€-í£]\", \" \", text)\n",
    "        morphs = self.okt.pos(text, stem=True)\n",
    "\n",
    "        tokens: List[str] = []\n",
    "        for word, pos in morphs:\n",
    "            if pos != \"Noun\":\n",
    "                continue\n",
    "            length_ok = len(word) >= self.min_word_length\n",
    "            if self.allow_single_char_noun and len(word) == 1:\n",
    "                length_ok = True\n",
    "            if not length_ok or word in self.stopwords or word.isdigit():\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "\n",
    "        self._dbg(\n",
    "            f\"Preprocessed {len(text)} chars â†’ {len(tokens)} tokens \"\n",
    "            f\"(sample: {tokens[:15]})\"\n",
    "        )\n",
    "        return tokens\n",
    "\n",
    "    # â”€â”€ íŒŒì¼ â†’ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ â”€â”€\n",
    "    def load_corpus_from_file(self, path: str, lines_per_doc: int = 5) -> List[str]:\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            lines = [ln.strip() for ln in f if ln.strip()]\n",
    "        docs = [\n",
    "            \" \".join(lines[i : i + lines_per_doc])\n",
    "            for i in range(0, len(lines), lines_per_doc)\n",
    "        ]\n",
    "        self._dbg(f\"Loaded {len(lines)} lines â†’ {len(docs)} docs\")\n",
    "        return docs\n",
    "\n",
    "    # â”€â”€ í•µì‹¬: TF-IDF ì¶”ì¶œ â”€â”€\n",
    "    def extract_difficult_words(\n",
    "        self,\n",
    "        terms_text: str,\n",
    "        daily_corpus_path: str,\n",
    "        *,\n",
    "        top_n: int = 20,\n",
    "        min_freq: int = 2,\n",
    "        lines_per_doc: int = 5,\n",
    "    ):\n",
    "        daily_raw = self.load_corpus_from_file(daily_corpus_path, lines_per_doc)\n",
    "        terms_tok = self.preprocess_text(terms_text)\n",
    "        daily_tok_list = [self.preprocess_text(doc) for doc in daily_raw]\n",
    "\n",
    "        docs = [\" \".join(terms_tok)] + [\" \".join(t) for t in daily_tok_list]\n",
    "\n",
    "        vec = TfidfVectorizer(\n",
    "            token_pattern=r\"(?u)\\b[\\wê°€-í£]+\\b\", max_features=5000, min_df=1\n",
    "        )\n",
    "        tfidf_mat = vec.fit_transform(docs)\n",
    "        feats = vec.get_feature_names_out()\n",
    "        terms_tfidf = tfidf_mat[0].toarray().flatten()\n",
    "        idf = vec.idf_\n",
    "\n",
    "        terms_cnt = Counter(terms_tok)\n",
    "        daily_cnt = Counter(tok for lst in daily_tok_list for tok in lst)\n",
    "\n",
    "        words = []\n",
    "        for i, w in enumerate(feats):\n",
    "            tfidf_val = terms_tfidf[i]\n",
    "            t_freq = terms_cnt.get(w, 0)\n",
    "            d_freq = daily_cnt.get(w, 0)\n",
    "\n",
    "            # â˜… í•„í„°: ì¼ìƒ ë¹ˆë„ 10 ì´ìƒì´ë©´ ì œì™¸ â˜…\n",
    "            if (\n",
    "                t_freq >= min_freq\n",
    "                and tfidf_val > 0\n",
    "                and d_freq < 10            # â† í•µì‹¬ ì¡°ê±´\n",
    "            ):\n",
    "                words.append((w, tfidf_val, t_freq, idf[i], t_freq, d_freq))\n",
    "\n",
    "        self._dbg(f\"Candidates kept after daily_freq<10: {len(words)}\")\n",
    "        words.sort(key=lambda x: x[1], reverse=True)\n",
    "        return words[:top_n]\n",
    "\n",
    "    # â”€â”€ ì‹¤í–‰ & ì¶œë ¥ â”€â”€\n",
    "    def analyze_and_display(\n",
    "        self,\n",
    "        terms_file: str,\n",
    "        corpus_file: str,\n",
    "        *,\n",
    "        top_n: int = 20,\n",
    "        min_freq: int = 2,\n",
    "        lines_per_doc: int = 5,\n",
    "    ) -> Optional[pd.DataFrame]:\n",
    "        terms_text = open(terms_file, encoding=\"utf-8\").read()\n",
    "\n",
    "        res = self.extract_difficult_words(\n",
    "            terms_text,\n",
    "            corpus_file,\n",
    "            top_n=top_n,\n",
    "            min_freq=min_freq,\n",
    "            lines_per_doc=lines_per_doc,\n",
    "        )\n",
    "        if not res:\n",
    "            print(\"No difficult words found (after daily_freq < 10 filter).\")\n",
    "            return None\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            res, columns=[\"word\", \"tfidf\", \"tf\", \"idf\", \"terms_freq\", \"daily_freq\"]\n",
    "        )\n",
    "        df[\"difficulty\"] = df[\"terms_freq\"] / (df[\"daily_freq\"] + 1)\n",
    "\n",
    "        # ë©”ì¸ í…Œì´ë¸”\n",
    "        print(\"=\" * 90)\n",
    "        print(f\"Top {len(df)} Difficult Words  (daily_freq < 10)\")\n",
    "        print(\"=\" * 90)\n",
    "        print(\n",
    "            f\"{'Rank':<4} {'Word':<20} {'TF-IDF':<9} {'TF':<5} \"\n",
    "            f\"{'IDF':<7} {'TermsFreq':<10} {'DailyFreq':<9} {'Difficulty':<10}\"\n",
    "        )\n",
    "        print(\"-\" * 90)\n",
    "        for i, r in df.iterrows():\n",
    "            print(\n",
    "                f\"{i+1:<4} {r.word:<20} {r.tfidf:<9.4f} {r.tf:<5} \"\n",
    "                f\"{r.idf:<7.4f} {r.terms_freq:<10} {r.daily_freq:<9} \"\n",
    "                f\"{r.difficulty:<10.2f}\"\n",
    "            )\n",
    "\n",
    "        # sparse spotlight(daily_freq == 0)\n",
    "        sparse = df[df[\"daily_freq\"] == 0]\n",
    "        if not sparse.empty:\n",
    "            print(\"\\nâ–º Sparse words (absent from daily corpus):\")\n",
    "            for w in sparse[\"word\"]:\n",
    "                print(\"  â€¢\", w)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "# â”€â”€ Driver â”€â”€\n",
    "def main() -> None:\n",
    "    ext = TermsDifficultWordsExtractor(\n",
    "        min_word_length=4, allow_single_char_noun=False, debug=True\n",
    "    )\n",
    "    ext.analyze_and_display(\n",
    "        terms_file=\"term2.txt\",\n",
    "        corpus_file=\"corpus.txt\",\n",
    "        top_n=15,\n",
    "        min_freq=1,\n",
    "        lines_per_doc=10,\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"Analysis completed.  See 'terms_extractor_debug.log' for details.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245ea90d-6eb5-4f7c-acb5-dc9c4641a2c1",
   "metadata": {},
   "source": [
    "ë„ë©”ì¸ ë¶„ë¥˜ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4169b7c6-8932-4d08-8501-9c13609db3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "# ============ 1. JSON -> CSV ë³€í™˜ í•¨ìˆ˜ ============\n",
    "def json_dir_to_csv(dir_path, output_csv_path, exclude_fields=None, label=None):\n",
    "    exclude_fields = set(exclude_fields) if exclude_fields else set()\n",
    "    rows = []\n",
    "\n",
    "    for fname in os.listdir(dir_path):\n",
    "        if fname.endswith('.json'):\n",
    "            with open(os.path.join(dir_path, fname), encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            clause_field = data.get('clauseField')\n",
    "            if clause_field and clause_field.isdigit():\n",
    "                if int(clause_field) in exclude_fields:\n",
    "                    continue\n",
    "                text = data.get('clauseArticle', [''])[0]\n",
    "                rows.append({'clauseField': int(clause_field), 'clauseArticle': text, 'label': label})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"CSV saved to {output_csv_path} (samples: {len(df)})\")\n",
    "\n",
    "# ============ 2. ClauseDataset (CSV ê¸°ë°˜) ============\n",
    "class ClauseDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.encodings = tokenizer(\n",
    "            self.df['clauseArticle'].tolist(),\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        self.labels = torch.tensor(self.df['label'].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# ============ 3. BertClassifier ì •ì˜ ============\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='kykim/bert-kor-base'):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        return self.classifier(cls_output)\n",
    "\n",
    "# ============ 4. í•™ìŠµ / í‰ê°€ í•¨ìˆ˜ ============\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device, return_all=False):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, mask)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    accuracy = sum([p == l for p, l in zip(all_preds, all_labels)]) / len(all_labels)\n",
    "\n",
    "    if return_all:\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        return accuracy, f1, cm\n",
    "    else:\n",
    "        return accuracy\n",
    "\n",
    "# ============ 5. ë©”ì¸ ì‹¤í–‰ë¶€ ============\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # ê²€ì¦ ë°ì´í„° clauseField (í•™ìŠµ ë°ì´í„°ì—ì„œ ì œì™¸í•  í•„ë“œ)\n",
    "    val_clause_fields = [1, 2, 3, 5, 6, 7, 8, 11, 25, 27, 28, 29, 30, 31, 32, 33, 39, 43]\n",
    "\n",
    "    # ê²½ë¡œ\n",
    "    base_train_dir  = '/content/ì•½ê´€ë°ì´í„°/TL_2.ì•½ê´€/TL_2.ì•½ê´€/1.Training/ë¼ë²¨ë§ë°ì´í„°/TL_2.ì•½ê´€'\n",
    "    base_val_dir = '/content/ì•½ê´€ë°ì´í„°/VL_2.ì•½ê´€/2.Validation/ë¼ë²¨ë§ë°ì´í„°/VL_2.ì•½ê´€'\n",
    "\n",
    "    # JSON -> CSV (í•„ìš”ì‹œ 1íšŒ ì‹¤í–‰)\n",
    "    json_dir_to_csv(os.path.join(base_train_dir, '01.ìœ ë¦¬'), 'train_good.csv', exclude_fields=val_clause_fields, label=1)\n",
    "    json_dir_to_csv(os.path.join(base_train_dir, '02.ë¶ˆë¦¬'), 'train_bad.csv', exclude_fields=val_clause_fields, label=0)\n",
    "    json_dir_to_csv(os.path.join(base_val_dir, '01.ìœ ë¦¬'), 'val_good.csv', label=1)\n",
    "    json_dir_to_csv(os.path.join(base_val_dir, '02.ë¶ˆë¦¬'), 'val_bad.csv', label=0)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('kykim/bert-kor-base')\n",
    "\n",
    "    train_dataset = ConcatDataset([\n",
    "        ClauseDataset('train_good.csv', tokenizer),\n",
    "        ClauseDataset('train_bad.csv', tokenizer)\n",
    "    ])\n",
    "    val_dataset = ConcatDataset([\n",
    "        ClauseDataset('val_good.csv', tokenizer),\n",
    "        ClauseDataset('val_bad.csv', tokenizer)\n",
    "    ])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, pin_memory=True, num_workers=4)\n",
    "\n",
    "    model = BertClassifier().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # í•™ìŠµ ë£¨í”„\n",
    "    epochs = 3\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nğŸŒŸ Epoch {epoch+1}\")\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        val_acc = evaluate(model, val_loader, device)\n",
    "        print(f\"âœ… Train Loss: {train_loss:.4f}, ğŸ” Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # ì „ì²´ í‰ê°€ ì§€í‘œ ì¶œë ¥\n",
    "    final_acc, final_f1, final_cm = evaluate(model, val_loader, device, return_all=True)\n",
    "    print(\"\\nğŸ“Š ì „ì²´ í‰ê°€ ì§€í‘œ:\")\n",
    "    print(f\"âœ… Accuracy: {final_acc:.4f}\")\n",
    "    print(f\"ğŸ¯ F1 Score: {final_f1:.4f}\")\n",
    "    print(\"ğŸ§® Confusion Matrix:\")\n",
    "    print(final_cm)\n",
    "\n",
    "    # ëª¨ë¸ ì €ì¥\n",
    "    torch.save(model.state_dict(), 'bert_clause_model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f056843",
   "metadata": {},
   "source": [
    "LLM í”„ë¡¬í”„íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf40f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. í™˜ê²½ ë³€ìˆ˜ & LLM\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise EnvironmentError(\"â— OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\",\n",
    "                 temperature=0.2,\n",
    "                 openai_api_key=api_key)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. ë„ë©”ì¸ ì •ì˜\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DOMAIN_CATS = [\n",
    "    \"A. ê¸ˆìœµê¸°ê´€\",\"B. ì „ìì§€ê¸‰Â·í•€í…Œí¬\",\"C. ë³´í—˜\",\"D. ì¦ê¶ŒÂ·íˆ¬ì\",\n",
    "    \"E. ìœ í†µÂ·ì‚¬ì´ë²„ëª°\",\"F. í”„ëœì°¨ì´ì¦ˆÂ·ê³µê¸‰Â·ë¶„ì–‘Â·ì‹ íƒ\",\"G. ë¶€ë™ì‚°Â·ì„ëŒ€ì°¨Â·ë¦¬ìŠ¤\",\n",
    "    \"H. ìš´ì†¡Â·ë¬¼ë¥˜\",\"I. ì—¬í–‰Â·ë ˆì €Â·ê²Œì„\",\"J. ìƒí™œì„œë¹„ìŠ¤\",\"K. ê¸°íƒ€ ê³„ì•½Â·ë³´ì¦\",\n",
    "]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. ì•½ê´€ ìš”ì•½ + ë„ë©”ì¸ ë¶„ë¥˜\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "summary_schema = ResponseSchema(\n",
    "    name=\"terms_summary\",\n",
    "    description=\"ì•½ê´€ ì „ë¬¸ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ìš”ì•½\"\n",
    ")\n",
    "domain_schema = ResponseSchema(\n",
    "    name=\"domains\",\n",
    "    description=\"ì¡°í•­ ëª©ë¡ê³¼ ë™ì¼í•œ ìˆœì„œë¡œ ë„ë©”ì¸ ë¬¸ìì—´ ë°°ì—´\"\n",
    ")\n",
    "parser = StructuredOutputParser.from_response_schemas([summary_schema, domain_schema])\n",
    "format_instr = parser.get_format_instructions()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"{format_instr}\n",
    "\n",
    "ì•½ê´€ ì „ë¬¸:\n",
    "{terms_text}\n",
    "\n",
    "ì¡°í•­ ëª©ë¡:\n",
    "{clauses}\n",
    "\n",
    "ì‘ì—…:\n",
    "1) ì•½ê´€ ì „ë¬¸ì„ ì†Œë¹„ì ê´€ì ì—ì„œ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ìš”ì•½\n",
    "2) ê° ì¡°í•­ì˜ ì‚°ì—…Â·ì„œë¹„ìŠ¤ ë¶„ì•¼(ì•„ë˜ 11ê°œ ì¤‘ í•˜ë‚˜)ë¥¼ ì„ íƒí•˜ì—¬ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´\n",
    "\n",
    "ì¹´í…Œê³ ë¦¬:\n",
    "{domain_list}\n",
    "\"\"\"\n",
    ")\n",
    "summary_chain = LLMChain(llm=llm, prompt=prompt, output_parser=parser)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. ì¡°í•­ ì²˜ë¦¬ í•¨ìˆ˜ (ì„ë² ë”© ì œê±°)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def process_clause(clause, label, domain):\n",
    "    prompt = f\"\"\"\n",
    "[ì¡°í•­ ì›ë¬¸]\n",
    "{clause}\n",
    "\n",
    "ì‘ì—…:\n",
    "1) ìœ„ ì¡°í•­ì´ ì†Œë¹„ìì—ê²Œ ì™œ {label}í•œì§€ 2~3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n",
    "2) {\"ë¶ˆë¦¬ ì¡°í•­ì„ ì†Œë¹„ìì—ê²Œ ìœ ë¦¬í•˜ê²Œ ê°œì •í•˜ì„¸ìš”. í˜•ì‹: ê°œì • ì „: / ê°œì • í›„:\" \n",
    "     if label == \"ë¶ˆë¦¬\" \n",
    "     else \"ìœ ë¦¬ ì¡°í•­ì€ ê·¸ëŒ€ë¡œ ë‘ê³ , ê°œì„ í•  ë¶€ë¶„ì´ ìˆìœ¼ë©´ ê°„ë‹¨íˆ ì œì•ˆí•˜ì„¸ìš”.\"}\n",
    "3) ê°€ëŠ¥í•˜ë©´ ì¼ë°˜ì ì¸ ê´€ë ¨ ë²•ë ¹ ì˜ˆì‹œë„ í•¨ê»˜ ì œê³µí•˜ì„¸ìš”.\n",
    "\"\"\"\n",
    "    llm_result = llm.predict(prompt).strip()\n",
    "\n",
    "\n",
    "    # SHAP ì„¤ëª… ëŒ€ì²´ ë˜ëŠ” ì œê±°\n",
    "    shap_info = {\"note\": \"SHAP ì„¤ëª… ìƒëµë¨ (ì„ë² ë”© ì œê±°ë¨)\"}\n",
    "\n",
    "    return {\n",
    "        \"domain\": domain,\n",
    "        \"llm_result\": llm_result,\n",
    "        \"law_refs\": [],\n",
    "        \"shap_explanation\": shap_info\n",
    "    }\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. ë©”ì¸ íŒŒì´í”„ë¼ì¸\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def run_terms_analysis(terms_text: str, clauses: List[str], labels: List[str]):\n",
    "    if len(clauses) != len(labels):\n",
    "        raise ValueError(\"clausesì™€ labels ê¸¸ì´ê°€ ë‹¤ë¦…ë‹ˆë‹¤.\")\n",
    "\n",
    "    # ìš”ì•½ + ë„ë©”ì¸ ë¶„ë¥˜\n",
    "    parsed = summary_chain.predict_and_parse(\n",
    "        terms_text=terms_text,\n",
    "        clauses=\"\\n\".join(f\"{i+1}) {c}\" for i, c in enumerate(clauses)),\n",
    "        format_instr=format_instr,\n",
    "        domain_list=\"\\n\".join(DOMAIN_CATS)\n",
    "    )\n",
    "    terms_summary = parsed[\"terms_summary\"]\n",
    "    domains = parsed[\"domains\"]\n",
    "\n",
    "    if len(domains) != len(clauses):\n",
    "        domains = [\"K. ê¸°íƒ€ ê³„ì•½Â·ë³´ì¦\"] * len(clauses)\n",
    "\n",
    "    clause_results = [\n",
    "        process_clause(c, l, d)\n",
    "        for c, l, d in zip(clauses, labels, domains)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"terms_summary\": terms_summary,\n",
    "        \"clause_results\": clause_results\n",
    "    }\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 9. ë°ëª¨\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    terms = Path(\"terms.txt\").read_text(encoding=\"utf-8\")\n",
    "    clauses_demo = [\n",
    "        \"ì œ22ì¡° (í™˜ë¶ˆ) íšŒì‚¬ëŠ” í•´ì§€ ì‹ ì²­ í›„ 7ì¼ ë‚´ í™˜ë¶ˆí•œë‹¤.\",\n",
    "        \"ì œ5ì¡° (ì²­ì•½ì² íšŒ) ê³ ê°ì€ ìƒí’ˆ ìˆ˜ë ¹ í›„ 7ì¼ ì´ë‚´ ì²­ì•½ì² íšŒ ê°€ëŠ¥í•˜ë‹¤.\"\n",
    "    ]\n",
    "    labels_demo = [\"ë¶ˆë¦¬\", \"ìœ ë¦¬\"]\n",
    "\n",
    "    report = run_terms_analysis(terms, clauses_demo, labels_demo)\n",
    "\n",
    "    print(\"\\nâ—† ì•½ê´€ ìš”ì•½ â—†\\n\", report[\"terms_summary\"])\n",
    "    for i, (r, lab) in enumerate(zip(report[\"clause_results\"], labels_demo), 1):\n",
    "        print(f\"\\nâ—‡ ì¡°í•­ {i} ({lab})\")\n",
    "        print(\"ë„ë©”ì¸:\", r[\"domain\"])\n",
    "        print(r[\"llm_result\"])\n",
    "        if r[\"law_refs\"]:\n",
    "            print(\"ê´€ë ¨ ë²•ë ¹:\", \", \".join(r[\"law_refs\"])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8eb75",
   "metadata": {},
   "source": [
    "ë°°í¬) í”„ë¡ íŠ¸ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import React, { useState } from 'react';\n",
    "import ReactMarkdown from 'react-markdown';\n",
    "import './App.css';\n",
    "\n",
    "// ê° ë¶„ì„ ê²°ê³¼ë¥¼ í‘œì‹œí•˜ê¸° ìœ„í•œ ë¶„ë¦¬ëœ ì»´í¬ë„ŒíŠ¸ë“¤\n",
    "\n",
    "// 1. TF-IDF ë¶„ì„ ê²°ê³¼ ì»´í¬ë„ŒíŠ¸ (ìˆ˜ì¹˜ ì œê±°)\n",
    "const TfidfResult = ({ result }) => (\n",
    "  <section className=\"result-section\">\n",
    "    <h2>ğŸ“š ì–´ë ¤ìš´ ë‹¨ì–´ ë° ìš©ì–´ ì„¤ëª…</h2>\n",
    "    {result.difficult_words && result.difficult_words.length > 0 ? (\n",
    "      <ul>\n",
    "        {result.difficult_words.map((item, index) => (\n",
    "          <li key={index}>\n",
    "            {/* ìš”êµ¬ì‚¬í•­ 4: TF-IDF ì ìˆ˜ ìˆ˜ì¹˜ë¥¼ ì œê±°í•©ë‹ˆë‹¤. */}\n",
    "            <strong>{item.word}:</strong> {item.definition}\n",
    "          </li>\n",
    "        ))}\n",
    "      </ul>\n",
    "    ) : (\n",
    "      <p>íŠ¹ë³„íˆ ì–´ë ¤ìš´ ë‹¨ì–´ëŠ” ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.</p>\n",
    "    )}\n",
    "  </section>\n",
    ");\n",
    "\n",
    "// 2. SHAP/BERT ë‹¨ì¼ ì¡°í•­ ì‹¬ì¸µ ë¶„ì„ ê²°ê³¼ ì»´í¬ë„ŒíŠ¸\n",
    "const ShapResult = ({ result }) => (\n",
    "  <section className=\"result-section\">\n",
    "    <h2>ğŸ’¡ AIì˜ ìƒì„¸ ì„¤ëª… ë° ì†”ë£¨ì…˜</h2>\n",
    "    <div className=\"summary-box\">\n",
    "      <h3>ğŸ“Œ AI íŒë‹¨ ìš”ì•½</h3>\n",
    "      <p>\n",
    "        AIê°€ ì´ ì¡°í•­ì„ <strong>'{result.prediction}'</strong>í•˜ë‹¤ê³  íŒë‹¨í–ˆìŠµë‹ˆë‹¤.<br/>\n",
    "        ì£¼ìš” íŒë‹¨ ê·¼ê±°ëŠ” <strong>\"{result.key_phrase}\"</strong> ì™€(ê³¼) ê´€ë ¨ëœ ë‚´ìš©ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\n",
    "      </p>\n",
    "    </div>\n",
    "    <div className=\"solution-box\">\n",
    "      <h3>ğŸ’¬ ìƒì„¸ í•´ì„¤</h3>\n",
    "      <ReactMarkdown children={result.llm_explanation} />\n",
    "    </div>\n",
    "    {/* ê´€ë ¨ í‚¤ì›Œë“œ ì„¹ì…˜ì€ TfidfResultì™€ ê²¹ì¹˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìƒëµí•˜ê±°ë‚˜ ë‹¤ë¥´ê²Œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. */}\n",
    "  </section>\n",
    ");\n",
    "\n",
    "// 3. ì „ì²´ ì•½ê´€ Top 3 í•„í„°ë§ ê²°ê³¼ ì»´í¬ë„ŒíŠ¸ (ìˆ˜ì¹˜ ì œê±°)\n",
    "const Top3Result = ({ result }) => (\n",
    "  <section className=\"result-section\">\n",
    "    <h2>ğŸ“Š ì „ì²´ ì•½ê´€ í•µì‹¬ ì¡°í•­ í•„í„°ë§</h2>\n",
    "    <p>ì´ {result.total_clauses_found}ê°œì˜ ì¡°í•­ ì¤‘ì—ì„œ ê°€ì¥ ì£¼ëª©í•  ë§Œí•œ ì¡°í•­ë“¤ì…ë‹ˆë‹¤.</p>\n",
    "    <div className=\"result-card danger\">\n",
    "      <h3>âŒ ê°€ì¥ ë¶ˆë¦¬í•œ Top 3 ì¡°í•­</h3>\n",
    "      {result.top_unfavorable_clauses.map((c, i) => (\n",
    "        <div key={`unfavorable-${i}`} className=\"clause-item\">\n",
    "          {/* ìš”êµ¬ì‚¬í•­ 4: í™•ë¥ (%) ìˆ˜ì¹˜ë¥¼ ì œê±°í•˜ê³  ì¡°í•­ í…ìŠ¤íŠ¸ë§Œ í‘œì‹œí•©ë‹ˆë‹¤. */}\n",
    "          <p>{c.text}</p>\n",
    "        </div>\n",
    "      ))}\n",
    "    </div>\n",
    "    <div className=\"result-card safe\">\n",
    "      <h3>âœ… ê°€ì¥ ìœ ë¦¬í•œ Top 3 ì¡°í•­</h3>\n",
    "      {result.top_favorable_clauses.map((c, i) => (\n",
    "        <div key={`favorable-${i}`} className=\"clause-item\">\n",
    "          {/* ìš”êµ¬ì‚¬í•­ 4: í™•ë¥ (%) ìˆ˜ì¹˜ë¥¼ ì œê±°í•˜ê³  ì¡°í•­ í…ìŠ¤íŠ¸ë§Œ í‘œì‹œí•©ë‹ˆë‹¤. */}\n",
    "          <p>{c.text}</p>\n",
    "        </div>\n",
    "      ))}\n",
    "    </div>\n",
    "  </section>\n",
    ");\n",
    "\n",
    "\n",
    "// ë©”ì¸ ì•± ì»´í¬ë„ŒíŠ¸\n",
    "function App() {\n",
    "  const [inputText, setInputText] = useState(\"\");\n",
    "  const [tfidfResult, setTfidfResult] = useState(null);\n",
    "  const [shapResult, setShapResult] = useState(null);\n",
    "  const [top3Result, setTop3Result] = useState(null);\n",
    "  const [isLoading, setIsLoading] = useState(false);\n",
    "  const [error, setError] = useState(null);\n",
    "\n",
    "  const handleComprehensiveAnalysis = async () => {\n",
    "    if (!inputText.trim()) {\n",
    "      setError(\"ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\");\n",
    "      return;\n",
    "    }\n",
    "    \n",
    "    setIsLoading(true);\n",
    "    setError(null);\n",
    "    setTfidfResult(null);\n",
    "    setShapResult(null);\n",
    "    setTop3Result(null);\n",
    "\n",
    "    try {\n",
    "      // Promise.allì„ ì‚¬ìš©í•´ 3ê°œì˜ APIë¥¼ ë™ì‹œì— í˜¸ì¶œ\n",
    "      const [tfidfResponse, shapResponse, top3Response] = await Promise.all([\n",
    "        fetch(\"http://localhost:8080/extract-difficult-words\", {\n",
    "          method: \"POST\", headers: { \"Content-Type\": \"application/json\" }, body: JSON.stringify({ text: inputText }),\n",
    "        }),\n",
    "        fetch(\"http://localhost:8080/analyze-clause-with-shap\", {\n",
    "          method: \"POST\", headers: { \"Content-Type\": \"application/json\" }, body: JSON.stringify({ text: inputText }),\n",
    "        }),\n",
    "        fetch(\"http://localhost:8080/analyze-full-terms-top3\", {\n",
    "          method: \"POST\", headers: { \"Content-Type\": \"application/json\" }, body: JSON.stringify({ full_text: inputText }),\n",
    "        }),\n",
    "      ]);\n",
    "\n",
    "      if (!tfidfResponse.ok || !shapResponse.ok || !top3Response.ok) {\n",
    "        throw new Error(\"í•˜ë‚˜ ì´ìƒì˜ ë¶„ì„ APIì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\");\n",
    "      }\n",
    "\n",
    "      const tfidfData = await tfidfResponse.json();\n",
    "      const shapData = await shapResponse.json();\n",
    "      const top3Data = await top3Response.json();\n",
    "      \n",
    "      setTfidfResult(tfidfData);\n",
    "      setShapResult(shapData);\n",
    "      setTop3Result(top3Data);\n",
    "\n",
    "    } catch (err) {\n",
    "      setError(err.message);\n",
    "    } finally {\n",
    "      setIsLoading(false);\n",
    "    }\n",
    "  };\n",
    "\n",
    "  return (\n",
    "    <div className=\"container\">\n",
    "      <header>\n",
    "        <h1>ğŸ§  AI ì•½ê´€ ì¢…í•© ë¶„ì„</h1>\n",
    "        <p>ì…ë ¥í•œ ì•½ê´€ì— ëŒ€í•´ 3ê°€ì§€ AI ë¶„ì„ ê²°ê³¼ë¥¼ í•œ ë²ˆì— ì œê³µí•©ë‹ˆë‹¤.</p>\n",
    "      </header>\n",
    "\n",
    "      <section className=\"input-section\">\n",
    "        <textarea\n",
    "          rows=\"10\"\n",
    "          value={inputText}\n",
    "          onChange={(e) => setInputText(e.target.value)}\n",
    "          placeholder=\"ì—¬ê¸°ì— ì•½ê´€ ì¡°í•­ ë˜ëŠ” ì•½ê´€ ì „ì²´ë¥¼ ì…ë ¥í•˜ì„¸ìš”...\"\n",
    "        />\n",
    "        <button onClick={handleComprehensiveAnalysis} disabled={isLoading}>\n",
    "          {isLoading ? \"ğŸ”„ ëª¨ë“  AI ë¶„ì„ ì¤‘...\" : \"ğŸ” ì¢…í•© ë¶„ì„í•˜ê¸°\"}\n",
    "        </button>\n",
    "      </section>\n",
    "      \n",
    "      {isLoading && <p className=\"loading-message\">AIê°€ ì•½ê´€ì„ ë‹¤ê°ë„ë¡œ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...</p>}\n",
    "      {error && <p className=\"error-message\">ì˜¤ë¥˜: {error}</p>}\n",
    "\n",
    "      {/* ëª¨ë“  ê²°ê³¼ê°€ ë„ì°©í–ˆì„ ë•Œë§Œ, ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì¬ë°°ì¹˜ëœ ìˆœì„œë¡œ ë Œë”ë§ */}\n",
    "      {tfidfResult && shapResult && top3Result && (\n",
    "        <div className=\"all-results-container\">\n",
    "          {/* ìš”êµ¬ì‚¬í•­ 1: Top3 ê²°ê³¼ë¥¼ ê°€ì¥ ë¨¼ì € í‘œì‹œ */}\n",
    "          <Top3Result result={top3Result} />\n",
    "          \n",
    "          {/* ìš”êµ¬ì‚¬í•­ 2: AI ìƒì„¸ ì„¤ëª…ì„ ë‘ ë²ˆì§¸ë¡œ í‘œì‹œ */}\n",
    "          <ShapResult result={shapResult} />\n",
    "          \n",
    "          {/* ìš”êµ¬ì‚¬í•­ 3: ë‹¨ì–´ ì„¤ëª…ì„ ë§ˆì§€ë§‰ìœ¼ë¡œ í‘œì‹œ */}\n",
    "          <TfidfResult result={tfidfResult} />\n",
    "        </div>\n",
    "      )}\n",
    "    </div>\n",
    "  );\n",
    "}\n",
    "\n",
    "export default App;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674744a",
   "metadata": {},
   "source": [
    "ë°°í¬) ë°±ì—”ë“œ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb79bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "# ===================================================================\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import xml.etree.ElementTree as ET\n",
    "import shap\n",
    "from collections import defaultdict, Counter\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "import openai\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.schema.runnable import RunnableSequence\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ===================================================================\n",
    "# 2. ì´ˆê¸° ì„¤ì • (FastAPI ì•±, API í‚¤, ëª¨ë¸ ë¡œë“œ)\n",
    "# ===================================================================\n",
    "app = FastAPI(\n",
    "    title=\"[ìµœì¢…] í†µí•© ì•½ê´€ ë¶„ì„ API\",\n",
    "    description=\"ëª¨ë“  ë¶„ì„ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ìµœì¢… ë²„ì „ì…ë‹ˆë‹¤.\",\n",
    "    version=\"5.0.0\"\n",
    ")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\", \"http://127.0.0.1:3000\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = \"open_ai_key\"\n",
    "\n",
    "NAVER_CLIENT_ID = \"naver_api\"\n",
    "\n",
    "NAVER_CLIENT_SECRET = \"naver_api_key\"\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"C:/Users/xison/Desktop/í…ë§ˆë°°í¬/ML/backend/model/domain_model\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.2, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# ===================================================================\n",
    "# 3. ëª¨ë“  ê¸°ëŠ¥ë³„ ë¡œì§ ë° í—¬í¼ í•¨ìˆ˜\n",
    "# ===================================================================\n",
    "\n",
    "# main.py íŒŒì¼ì˜ ë‹¤ë¥¸ í—¬í¼ í•¨ìˆ˜ë“¤ê³¼ í•¨ê»˜ ì•„ë˜ í•¨ìˆ˜ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "def split_clauses(text: str) -> List[str]:\n",
    "    \"\"\"'ì œ nì¡°' íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ì—ì„œ ì¡°í•­ë“¤ì„ ë¶„ë¦¬\"\"\"\n",
    "    # 'ì œ nì¡°(ì œëª©)' í˜•ì‹ê¹Œì§€ í¬í•¨í•˜ì—¬ ë¶„ë¦¬í•˜ëŠ” ì •ê·œì‹\n",
    "    pattern = re.compile(r'(ì œ\\s*\\d+\\s*ì¡°\\s*(?:\\([^)]*\\))?[^\\n]*)')\n",
    "    \n",
    "    # ì •ê·œì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    # ë¶„ë¦¬ëœ ì¡°í•­ë“¤ì„ ì¬ì¡°í•©\n",
    "    # parts ë¦¬ìŠ¤íŠ¸ëŠ” [ì „ë¬¸, ì œ1ì¡°, ì œ1ì¡°ë‚´ìš©, ì œ2ì¡°, ì œ2ì¡°ë‚´ìš©, ...] í˜•ì‹ìœ¼ë¡œ ë‚˜ë‰¨\n",
    "    clauses = []\n",
    "    if len(parts) > 1:\n",
    "        # ì²« ë²ˆì§¸ ìš”ì†ŒëŠ” 'ì œ1ì¡°' ì´ì „ì˜ ì„œë¬¸ì´ë¯€ë¡œ ì œì™¸í•˜ê³  ì‹œì‘\n",
    "        for i in range(1, len(parts), 2):\n",
    "            if i + 1 < len(parts) and parts[i+1].strip():\n",
    "                # \"ì œnì¡°(ì œëª©)\" + \"ë‚´ìš©\" ì„ í•©ì³ì„œ í•˜ë‚˜ì˜ ì¡°í•­ìœ¼ë¡œ ë§Œë“¦\n",
    "                full_clause = parts[i].strip() + \"\\n\" + parts[i+1].strip()\n",
    "                clauses.append(full_clause)\n",
    "\n",
    "    # ë§Œì•½ 'ì œ nì¡°' íŒ¨í„´ì´ í•˜ë‚˜ë„ ì—†ë‹¤ë©´, ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì¼ ì¡°í•­ìœ¼ë¡œ ê°„ì£¼\n",
    "    if not clauses and text.strip():\n",
    "        return [text.strip()]\n",
    "        \n",
    "    return clauses\n",
    "\n",
    "# --- ê¸°ëŠ¥ A: TF-IDF ë¡œì§ ---\n",
    "class TermsDifficultWordsExtractor:\n",
    "    def __init__(self, min_word_length: int = 2):\n",
    "        self.okt = Okt()\n",
    "        self.min_word_length = min_word_length\n",
    "        self.stopwords = {\"ì€\", \"ëŠ”\", \"ì´\", \"ê°€\", \"ì„\", \"ë¥¼\", \"ì—\", \"ì˜\", \"ì™€\", \"ê³¼\", \"ë„\", \"ë¡œ\", \"ìœ¼ë¡œ\", \"ì—ì„œ\", \"íšŒì‚¬\", \"ì„œë¹„ìŠ¤\", \"íšŒì›\", \"ì´ìš©\", \"ì•½ê´€\", \"ê³ ê°\", \"ì‚¬ì´íŠ¸\", \"ë³¸\", \"ë‹¹ì‚¬\", \"ì‚¬ìš©\", \"ì •ë³´\", \"ê´€ë ¨\", \"ì œê³µ\", \"ìœ„\", \"ì´í•˜\", \"ê²½ìš°\", \"ë•Œ\", \"ë‚´ìš©\", \"ëª©ì \", \"ì¡°í•­\"}\n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        text = re.sub(r\"[^\\w\\sê°€-í£]\", \" \", text)\n",
    "        morphs = self.okt.pos(text, stem=True)\n",
    "        return [word for word, pos in morphs if pos == \"Noun\" and len(word) >= self.min_word_length and word not in self.stopwords and not word.isdigit()]\n",
    "    def load_corpus_from_file(self, path: str, lines_per_doc: int = 5) -> List[str]:\n",
    "        try:\n",
    "            with open(path, encoding=\"utf-8\") as f: lines = [ln.strip() for ln in f if ln.strip()]\n",
    "            return [\" \".join(lines[i : i + lines_per_doc]) for i in range(0, len(lines), lines_per_doc)]\n",
    "        except FileNotFoundError: return []\n",
    "    def extract_difficult_words(self, terms_text: str, daily_corpus_path: str, top_n: int = 5):\n",
    "        daily_raw = self.load_corpus_from_file(daily_corpus_path)\n",
    "        terms_tok = self.preprocess_text(terms_text)\n",
    "        daily_tok_list = [self.preprocess_text(doc) for doc in daily_raw]\n",
    "        docs = [\" \".join(terms_tok)] + [\" \".join(t) for t in daily_tok_list]\n",
    "        vec = TfidfVectorizer(token_pattern=r\"(?u)\\b[\\wê°€-í£]+\\b\", max_features=5000, min_df=1)\n",
    "        tfidf_mat = vec.fit_transform(docs)\n",
    "        feats = vec.get_feature_names_out()\n",
    "        terms_tfidf = tfidf_mat[0].toarray().flatten()\n",
    "        terms_cnt = Counter(terms_tok)\n",
    "        daily_cnt = Counter(tok for lst in daily_tok_list for tok in lst)\n",
    "        words = [{\"word\": w, \"tfidf\": terms_tfidf[i], \"terms_freq\": terms_cnt.get(w, 0), \"daily_freq\": daily_cnt.get(w, 0)} for i, w in enumerate(feats) if terms_cnt.get(w, 0) >= 1 and terms_tfidf[i] > 0 and daily_cnt.get(w, 0) < 10]\n",
    "\n",
    "        words = [word_data for word_data in words if len(word_data[\"word\"]) == 4]\n",
    "        words.sort(key=lambda x: x[\"tfidf\"], reverse=True)\n",
    "        return words[:top_n]\n",
    "\n",
    "\n",
    "tfidf_extractor = TermsDifficultWordsExtractor()\n",
    "\n",
    "# --- ê¸°ëŠ¥ B, C, D ê³µìš© í—¬í¼ í•¨ìˆ˜ ---\n",
    "def predict_clause_probabilities(text: str) -> dict:\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=1)[0].cpu().tolist()\n",
    "    return {\"prob_unfavorable\": probs[0], \"prob_favorable\": probs[1]}\n",
    "\n",
    "#ë„¤ì´ë²„ api ë°±ê³¼ì‚¬ì „ í˜¸ì¶œ ê¸°ëŠ¥\n",
    "def search_naver_encyc(query: str):\n",
    "    encText = urllib.parse.quote(query)\n",
    "    url = f\"https://openapi.naver.com/v1/search/encyc.xml?query={encText}\"\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\", NAVER_CLIENT_ID)\n",
    "    request.add_header(\"X-Naver-Client-Secret\", NAVER_CLIENT_SECRET)\n",
    "    \n",
    "    try:\n",
    "        with urllib.request.urlopen(request, timeout=5) as response:\n",
    "            if response.getcode() == 200:\n",
    "                item = ET.fromstring(response.read()).find(\"channel/item\")\n",
    "                if item is not None and item.find(\"description\") is not None and item.find(\"description\").text:\n",
    "                    return re.sub('<.+?>', '', item.find(\"description\").text).strip()\n",
    "        return \"ì‚¬ì „ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    except Exception as e: return f\"ì‚¬ì „ API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜({type(e).__name__}) ë°œìƒ\"\n",
    "\n",
    "# --- ê¸°ëŠ¥ B: SHAP/BERT ê°œì„ ëœ ë¶„ì„ ë¡œì§ ---\n",
    "class BertWrapper:\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model, self.tokenizer, self.device = model, tokenizer, device\n",
    "    def __call__(self, texts):\n",
    "        inputs = self.tokenizer(list(texts), return_tensors='pt', padding=True, truncation=True, max_length=128, add_special_tokens=True).to(self.device)\n",
    "        with torch.no_grad(): return F.softmax(self.model(**inputs).logits, dim=1).cpu().numpy()\n",
    "explainer = shap.Explainer(BertWrapper(model, tokenizer, device), shap.maskers.Text(tokenizer))\n",
    "\n",
    "def analyze_clause_with_shap(text: str):\n",
    "    probabilities = list(predict_clause_probabilities(text).values())\n",
    "    label_idx = torch.argmax(torch.tensor(probabilities)).item()\n",
    "    label_text = 'ìœ ë¦¬' if label_idx == 1 else 'ë¶ˆë¦¬'\n",
    "    shap_values = explainer([text])\n",
    "    encoding = tokenizer(text, return_offsets_mapping=True, return_tensors='pt', truncation=True, max_length=128)\n",
    "    tokens, offsets, values = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]), encoding['offset_mapping'][0].tolist(), shap_values.values[0][:, label_idx]\n",
    "    words, scores, current_word, current_score, prev_end = [], [], '', 0.0, -1\n",
    "    for token, (start, end), score in zip(tokens, offsets, values):\n",
    "        if token in ['[CLS]', '[SEP]'] or start == end: continue\n",
    "        if start != prev_end and current_word:\n",
    "            words.append(current_word); scores.append(current_score); current_word, current_score = '', 0.0\n",
    "        current_word += text[start:end]; current_score += score; prev_end = end\n",
    "    if current_word: words.append(current_word); scores.append(current_score)\n",
    "    word_score_dict = defaultdict(float, {w.strip(): s for w, s in zip(words, scores)})\n",
    "    top_words = [w for w, _ in sorted(word_score_dict.items(), key=lambda x: abs(x[1]), reverse=True) if len(w)>1][:4]\n",
    "    sentence_words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text)\n",
    "    seen, phrases = set(), []\n",
    "    for i in range(len(sentence_words)):\n",
    "        for j in range(i + 1, min(len(sentence_words), i + 8) + 1):\n",
    "            chunk = ' '.join(sentence_words[i:j]).strip()\n",
    "            if chunk in seen or not any(w in chunk for w in top_words): continue\n",
    "            seen.add(chunk)\n",
    "            phrases.append((chunk, sum(word_score_dict.get(w, 0.0) for w in chunk.split() if w in word_score_dict)))\n",
    "    phrases.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    awkward_key_phrase = phrases[0][0] if phrases else \"í•µì‹¬ êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ\"\n",
    "    return {\"prediction\": label_text, \"awkward_key_phrase\": awkward_key_phrase, \"top_words\": top_words}\n",
    "\n",
    "# --- ê¸°ëŠ¥ C: LangChain ë¡œì§ ---\n",
    "DOMAIN_CATS = [\"A. ê¸ˆìœµê¸°ê´€\",\"B. ì „ìì§€ê¸‰Â·í•€í…Œí¬\",\"C. ë³´í—˜\",\"D. ì¦ê¶ŒÂ·íˆ¬ì\",\"E. ìœ í†µÂ·ì‚¬ì´ë²„ëª°\",\"F. í”„ëœì°¨ì´ì¦ˆÂ·ê³µê¸‰Â·ë¶„ì–‘Â·ì‹ íƒ\",\"G. ë¶€ë™ì‚°Â·ì„ëŒ€ì°¨Â·ë¦¬ìŠ¤\",\"H. ìš´ì†¡Â·ë¬¼ë¥˜\",\"I. ì—¬í–‰Â·ë ˆì €Â·ê²Œì„\",\"J. ìƒí™œì„œë¹„ìŠ¤\",\"K. ê¸°íƒ€ ê³„ì•½Â·ë³´ì¦\"]\n",
    "summary_schema, domain_schema = ResponseSchema(name=\"terms_summary\", description=\"ì•½ê´€ ì „ë¬¸ ìš”ì•½\"), ResponseSchema(name=\"domains\", description=\"ì¡°í•­ë³„ ë„ë©”ì¸ ë¬¸ìì—´ ë°°ì—´\")\n",
    "parser = StructuredOutputParser.from_response_schemas([summary_schema, domain_schema])\n",
    "prompt_template = ChatPromptTemplate.from_template(\"\"\"{format_instr}\\n\\nì•½ê´€ ì „ë¬¸:\\n{terms_text}\\n\\nì¡°í•­ ëª©ë¡:\\n{clauses}\\n\\nì‘ì—…:\\n1) ì•½ê´€ ì „ë¬¸ì„ ìš”ì•½í•˜ì„¸ìš”.\\n2) ê° ì¡°í•­ì˜ ì‚°ì—… ë¶„ì•¼ë¥¼ ì•„ë˜ ì¹´í…Œê³ ë¦¬ ì¤‘ì—ì„œ ì„ íƒí•˜ì—¬ ë‚˜ì—´í•˜ì„¸ìš”.\\n\\nì¹´í…Œê³ ë¦¬:\\n{domain_list}\"\"\")\n",
    "summary_chain = prompt_template | llm | parser\n",
    "\n",
    "def process_clause_with_langchain(clause: str, label: str, domain: str):\n",
    "    prompt_text = f\"\"\"[ì¡°í•­ ì›ë¬¸]\n",
    "{clause}\n",
    "\n",
    "[ë¶„ì„ ì‘ì—…]\n",
    "ë‹¹ì‹ ì€ ì†Œë¹„ìì˜ ì…ì¥ì—ì„œ ì•½ê´€ì„ ë¶„ì„í•˜ëŠ” AI ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "1. ìœ„ ì¡°í•­ì´ ì™œ ì†Œë¹„ìì—ê²Œ '{label}'í•œì§€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ 2~3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n",
    "2. {'ë¶ˆë¦¬ ì¡°í•­ì„ ì†Œë¹„ìì—ê²Œ ìœ ë¦¬í•˜ê²Œ ê°œì •í•˜ëŠ” ì•ˆì„ ì œì‹œí•˜ì„¸ìš”. \"ê°œì • ì œì•ˆ:\" ì´ë¼ëŠ” ì œëª©ìœ¼ë¡œ ì‹œì‘í•´ì£¼ì„¸ìš”.' if label == \"ë¶ˆë¦¬\" else 'í•´ë‹¹ ì¡°í•­ì€ ì†Œë¹„ìì—ê²Œ ìœ ë¦¬í•˜ì§€ë§Œ, ë” ê°œì„ í•  ë¶€ë¶„ì´ ìˆë‹¤ë©´ ì œì•ˆí•´ì£¼ì„¸ìš”. \"ê°œì„  ì œì•ˆ:\" ì´ë¼ëŠ” ì œëª©ìœ¼ë¡œ ì‹œì‘í•´ì£¼ì„¸ìš”.'}\n",
    "3. ì´ ì¡°í•­ê³¼ ê´€ë ¨ëœ ì¼ë°˜ì ì¸ ë²•ë¥ (ì˜ˆ: ì•½ê´€ê·œì œë²•, ì „ììƒê±°ë˜ë²• ë“±)ì´ ìˆë‹¤ë©´ ì–´ë–¤ ê²ƒì¸ì§€ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "    llm_result = llm.predict(prompt_text).strip()\n",
    "    return {\"domain\": domain, \"original_clause\": clause, \"label\": label, \"analysis_and_suggestion\": llm_result}\n",
    "\n",
    "\n",
    "def run_terms_analysis(terms_text: str, clauses: List[str], labels: List[str]):\n",
    "    try:\n",
    "        parsed = summary_chain.invoke({\"terms_text\": terms_text, \"clauses\": \"\\n\".join(f\"{i+1}) {c}\" for i, c in enumerate(clauses)), \"format_instr\": parser.get_format_instructions(), \"domain_list\": \"\\n\".join(DOMAIN_CATS)})\n",
    "        terms_summary, domains = parsed[\"terms_summary\"], parsed[\"domains\"]\n",
    "    except Exception as e:\n",
    "        terms_summary, domains = f\"ìš”ì•½/ë„ë©”ì¸ ë¶„ë¥˜ ì‹¤íŒ¨: {e}\", [\"K. ê¸°íƒ€ ê³„ì•½Â·ë³´ì¦\"] * len(clauses)\n",
    "    if len(domains) != len(clauses): domains = [\"K. ê¸°íƒ€ ê³„ì•½Â·ë³´ì¦\"] * len(clauses)\n",
    "    return {\"terms_summary\": terms_summary, \"clause_results\": [process_clause_with_langchain(c, l, d) for c, l, d in zip(clauses, labels, domains)]}\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 5. FastAPI Pydantic ëª¨ë¸ ë° ì—”ë“œí¬ì¸íŠ¸\n",
    "# ===================================================================\n",
    "\n",
    "# --- ì…ë ¥ ëª¨ë¸ ---\n",
    "class TextIn(BaseModel): text: str\n",
    "class FullTermsIn(BaseModel): full_text: str\n",
    "class LangchainIn(BaseModel): terms_text: str; clauses: List[str]; labels: List[str]\n",
    "\n",
    "# --- ì—”ë“œí¬ì¸íŠ¸ ---\n",
    "\n",
    "@app.post(\"/extract-difficult-words\", summary=\"[ê¸°ëŠ¥ 1] TF-IDF ì–´ë ¤ìš´ ë‹¨ì–´ ì¶”ì¶œ (ê¸°ì¡´ ê¸°ëŠ¥)\")\n",
    "async def endpoint_extract_difficult_words(payload: TextIn):\n",
    "    try:\n",
    "        difficult_words = tfidf_extractor.extract_difficult_words(payload.text, \"corpus.txt\", top_n=5)\n",
    "        for word_data in difficult_words: word_data[\"definition\"] = search_naver_encyc(word_data[\"word\"])\n",
    "        return {\"difficult_words\": difficult_words}\n",
    "    except Exception as e: raise HTTPException(500, f\"TF-IDF ë¶„ì„ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "@app.post(\"/analyze-clause-with-shap\", summary=\"[ê¸°ëŠ¥ 2] ë‹¨ì¼ ì¡°í•­ ì‹¬ì¸µ ë¶„ì„ (ê°œì„ ë¨)\")\n",
    "async def endpoint_analyze_clause_with_shap(payload: TextIn):\n",
    "    analysis = analyze_clause_with_shap(payload.text)\n",
    "    natural_phrase_prompt = f\"ë‹¤ìŒ ë¬¸ì¥ì€ ì»´í“¨í„° ë¶„ì„ ê²°ê³¼ë¡œ ì–´ìƒ‰í•©ë‹ˆë‹¤. í•µì‹¬ ì˜ë¯¸ëŠ” ìœ ì§€í•˜ë˜, ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹¤ë“¬ì–´ ì£¼ì„¸ìš”: '{analysis['awkward_key_phrase']}'\"\n",
    "    natural_key_phrase = llm.predict(natural_phrase_prompt)\n",
    "    explanation_prompt = f\"\"\"[ë¶„ì„ ëŒ€ìƒ ì¡°í•­]\\n\"{payload.text}\"\\n\\n[AI ë¶„ì„ ê²°ê³¼]\\n- ì´ ì¡°í•­ì€ '{analysis['prediction']}'í•˜ë‹¤ê³  íŒë‹¨ë©ë‹ˆë‹¤.\\n- íŒë‹¨ ê·¼ê±°ëŠ” '{natural_key_phrase}' ë‚´ìš©ì…ë‹ˆë‹¤.\\n\\n[ìš”ì²­ ì‘ì—…]\\nAI ë²•ë¥  ì „ë¬¸ê°€ë¡œì„œ, ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ í•­ëª©ì— ëŒ€í•´ Markdown í˜•ì‹ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\\n\\n- **í•µì‹¬ ë¬¸ì œì /í˜œíƒ**:\\n- **ì†Œë¹„ìì—ê²Œ ë¯¸ì¹˜ëŠ” ì˜í–¥**:\\n- **ê¶Œì¥ ëŒ€ì‘ ë°©ì•ˆ**:\"\"\"\n",
    "    llm_explanation = llm.predict(explanation_prompt)\n",
    "    definitions = [{\"word\": w, \"definition\": search_naver_encyc(w)} for w in analysis['top_words']]\n",
    "    return {\"prediction\": analysis['prediction'], \"key_phrase\": natural_key_phrase, \"llm_explanation\": llm_explanation, \"keywords_definitions\": definitions}\n",
    "\n",
    "@app.post(\"/analyze-terms-with-langchain\", summary=\"[ê¸°ëŠ¥ 3] LangChain ì „ì²´ ì•½ê´€ ë¶„ì„ (ê¸°ì¡´ ê¸°ëŠ¥)\")\n",
    "async def endpoint_analyze_terms_with_langchain(payload: LangchainIn):\n",
    "    if len(payload.clauses) != len(payload.labels): raise HTTPException(400, \"clausesì™€ labels ê°œìˆ˜ ë¶ˆì¼ì¹˜\")\n",
    "    try: return run_terms_analysis(payload.terms_text, payload.clauses, payload.labels)\n",
    "    except Exception as e: raise HTTPException(500, f\"LangChain ë¶„ì„ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "@app.post(\"/analyze-full-terms-top3\", summary=\"[ê¸°ëŠ¥ 4] ì „ì²´ ì•½ê´€ Top 3 í•„í„°ë§ (ì‹ ê·œ ì¶”ê°€)\")\n",
    "async def endpoint_analyze_full_terms_top3(payload: FullTermsIn):\n",
    "    clauses = split_clauses(payload.full_text)\n",
    "    if not clauses: raise HTTPException(400, \"ë¶„ì„í•  ì¡°í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\")\n",
    "    all_results = [dict(text=c, **predict_clause_probabilities(c)) for c in clauses]\n",
    "    top_favorable = sorted(all_results, key=lambda x: x['prob_favorable'], reverse=True)[:3]\n",
    "    top_unfavorable = sorted(all_results, key=lambda x: x['prob_unfavorable'], reverse=True)[:3]\n",
    "    return {\"total_clauses_found\": len(clauses), \"top_favorable_clauses\": top_favorable, \"top_unfavorable_clauses\": top_unfavorable}\n",
    "\n",
    "@app.get(\"/\", summary=\"ì„œë²„ ìƒíƒœ í™•ì¸\")\n",
    "def read_root(): return {\"status\": \"ì•½ê´€ ë¶„ì„ API ì„œë²„ v5.0 (ëª¨ë“  ê¸°ëŠ¥ í†µí•©)ì´ ì •ìƒ ë™ì‘ ì¤‘ì…ë‹ˆë‹¤.\"} "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
